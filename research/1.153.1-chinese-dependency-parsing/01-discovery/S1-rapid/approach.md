# S1-Rapid: Approach

## Philosophy
"Popular libraries exist for a reason" - speed-focused, ecosystem-driven discovery for quick decision-making.

## Methodology

### Search Strategy
1. **Primary search**: Official library documentation and GitHub repositories
2. **Ecosystem indicators**: GitHub stars, PyPI downloads, community activity
3. **Quick technical assessment**: Installation ease, basic API patterns, output formats
4. **Time constraint**: 5-10 minute read per library

### Selection Criteria
Libraries were selected based on:
- **Relevance**: Explicit Chinese dependency parsing support
- **Adoption**: Evidence of real-world usage (GitHub stars, documentation quality)
- **Accessibility**: Available as pip/npm installable packages
- **Recency**: Active maintenance or established stability

### Libraries Evaluated

1. **Universal Dependencies (UD)** - Framework and treebank collection
2. **Stanford CoreNLP** - Java-based NLP suite with Chinese support
3. **HanLP** - Modern PyTorch/TensorFlow Chinese NLP library
4. **Stanza** - Stanford's neural Python NLP toolkit
5. **LTP** - Harbin Institute of Technology's Chinese-focused platform

## What S1 Reveals

S1 provides:
- Quick landscape overview for time-constrained decisions
- Identification of major players in Chinese dependency parsing
- Basic technical feasibility assessment
- Foundation for deeper analysis in S2-S4

## What S1 Doesn't Cover

- Performance benchmarks (see S2)
- Specific use case recommendations (see S3)
- Long-term strategic considerations (see S4)
- Installation guides (out of scope)
