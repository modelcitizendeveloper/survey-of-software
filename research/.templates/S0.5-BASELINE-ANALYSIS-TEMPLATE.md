# S0.5: Baseline Analysis Template

**Experiment:** [Experiment ID]
**Category:** [Category Name]
**Date:** [Date]
**Duration:** 30-45 minutes

---

## Purpose

Establish the DIY baseline and volume thresholds BEFORE researching external tools. This ensures:
1. You know if you need a tool at all
2. You understand the cost of building yourself
3. You can make the Path 1 (DIY) vs Path 2 (Open Standard) vs Path 3 (Managed) decision

---

## 1. DIY Implementation Analysis

### Code Complexity Estimate
- **Lines of code:** [Estimate: 50 / 500 / 5,000+]
- **Language/Stack:** [Python/Node/Go + database technology]
- **Dependencies:** [External libraries needed]

### Time Investment
- **Initial setup:** [Hours to first working version]
- **Full features:** [Hours to production-ready with basic UI]
- **Monthly maintenance:** [Hours/month ongoing]

### Feature Capabilities

**What DIY CAN achieve:**
- [Feature 1: e.g., basic event logging]
- [Feature 2: e.g., simple aggregations]
- [Feature 3: e.g., CSV exports]

**What DIY CANNOT achieve (or very expensive):**
- [Feature 1: e.g., real-time dashboards]
- [Feature 2: e.g., advanced visualizations]
- [Feature 3: e.g., multi-user access control]

### Technical Risks
- **Scaling ceiling:** [At what volume does DIY break? 1K/10K/100K events?]
- **Data quality:** [Will you miss edge cases? Browser compatibility?]
- **Security:** [Authentication, data privacy, PII scrubbing needed?]

### Real-World Examples
- **Example 1:** [Company/project using DIY approach]
- **Example 2:** [GitHub repos demonstrating DIY implementation]
- **Code snippet:** [Link to reference implementation]

**Verdict:**
- [ ] DIY is SIMPLE (2-4 hours, <200 lines, no maintenance)
- [ ] DIY is MODERATE (10-20 hours, 500-1K lines, 2-5 hrs/month maintenance)
- [ ] DIY is COMPLEX (50+ hours, 5K+ lines, 10+ hrs/month maintenance)

---

## 2. Open Standards Evaluation

### Standards Search
Check for industry standards from:
- **CNCF (Cloud Native Computing Foundation):** [Graduated/Incubating projects]
- **W3C (World Wide Web Consortium):** [Specifications]
- **IETF (Internet Engineering Task Force):** [RFCs]
- **De-facto standards:** [Widely adopted but not official]

### Standard Found: [Name or "NONE"]

**If standard exists:**
- **Name:** [e.g., OpenTelemetry, Prometheus format]
- **Maturity:** [Sandbox / Incubating / Graduated]
- **Instrumentation libraries:** [Languages supported]
- **Backend compatibility:** [Number of providers supporting standard]
- **Lock-in risk:** [Zero - can switch backends via config]

**If NO standard exists:**
- **Why not?** [Too new / too fragmented / proprietary moats]
- **De-facto alternatives:** [Segment API, custom schemas]
- **Portability options:** [CSV export, data warehouse integration]

**Verdict:**
- [ ] Path 2 (Open Standard) IS AVAILABLE → Recommend by default
- [ ] Path 2 (Open Standard) NOT AVAILABLE → Choose Path 1 or Path 3

---

## 3. Volume Threshold Analysis

### Event Volume Gates

Define three volume tiers:

**Tier 1: <[LOW_THRESHOLD] events/month**
- **Debugging time:** ~[X hours/month]
- **DIY verdict:** RECOMMENDED (learning > efficiency)
- **Tool verdict:** DEFER (premature optimization)
- **Rationale:** At low volume, debugging IS learning about your system

**Tier 2: [LOW_THRESHOLD] - [HIGH_THRESHOLD] events/month**
- **Debugging time:** ~[X hours/month]
- **DIY verdict:** ACCEPTABLE (with basic UI/tooling)
- **Tool verdict:** OPTIONAL (depends on founder values)
- **Rationale:** Hybrid approach (DIY + self-hosted UI)

**Tier 3: >[HIGH_THRESHOLD] events/month**
- **Debugging time:** ~[X hours/month] (unsustainable)
- **DIY verdict:** NOT RECOMMENDED (debugging becomes waste)
- **Tool verdict:** JUSTIFIED (efficiency > learning)
- **Rationale:** Tool aggregation/filtering saves significant time

### Typical Thresholds by Category

**Application Monitoring (Errors):**
- <100 errors/month: DIY logs (2-3 hrs/month debugging)
- 100-500 errors: OpenTelemetry + Jaeger (self-hosted UI)
- >500 errors: Sentry (managed, shared dashboards)

**Web Analytics (Pageviews):**
- <1K pageviews/month: DIY SQLite
- 1K-100K pageviews: Managed privacy tools (Plausible $9-19/mo)
- >100K pageviews: Managed or self-hosted Matomo

**Product Analytics (Events):**
- <500 events/month: DIY event tables + SQL
- 500-1M events: PostHog free tier (managed)
- >1M events: PostHog paid or self-hosted

**Uptime Monitoring (Checks):**
- ANY volume: Managed service (DIY never worth it - break-even: never)

---

## 4. Founder Archetype Assessment

### Three Founder Archetypes

Check which profile matches your situation:

**Archetype 1: Build to Learn (Solo Founder, Pre-PMF)**
- [ ] Solo founder or <3 person team
- [ ] Pre-product-market-fit (exploring, pivoting)
- [ ] Founder explicitly values learning/control
- [ ] Privacy/control is founder identity
- [ ] <100 events/month volume

**Recommended Path:** Path 1 (DIY) - build yourself, own the stack
**Rationale:** At low volume, debugging = product discovery. Tools displace learning.

**Archetype 2: Build to Last (Architect, Values Flexibility)**
- [ ] Want to defer vendor choice
- [ ] Value portability over convenience
- [ ] Industry-standard skills valued (hiring, community)
- [ ] Willing to invest 2-4 extra hours upfront for 20-40 hour future savings

**Recommended Path:** Path 2 (Open Standard) - OpenTelemetry, Prometheus, etc.
**Rationale:** Instrument once, switch backends later (zero vendor lock-in)

**Archetype 3: Build to Scale (VC-Backed, PMF Validated)**
- [ ] >500 events/month (debugging is waste)
- [ ] Team coordination needed (5+ people)
- [ ] Time-constrained (VC-backed, aggressive growth)
- [ ] PMF validated (optimizing, not exploring)
- [ ] Operational efficiency > cost optimization

**Recommended Path:** Path 3 (Managed) - Sentry, Plausible, PostHog
**Rationale:** Focus on product growth, outsource infrastructure complexity

---

## 5. Tools-as-Learning-Displacement Check

### When Tools REMOVE Value (Resist Adoption)

Check for these anti-patterns:

**Scale Mismatch:**
- [ ] "Semi-truck to deliver pizza" (enterprise tool for side project)
- [ ] Tool cost >10% of product revenue
- [ ] Team of 1-2 using tool designed for teams of 50+

**Learning Displacement:**
- [ ] <100 events/month (errors/pageviews/etc.)
- [ ] Solo founder where debugging teaches about user behavior
- [ ] Tool efficiency removes valuable feedback loops

**Values Misalignment:**
- [ ] Founder explicitly said "I prefer control" or "Don't trust external services"
- [ ] Privacy/self-reliance is founder's competitive advantage
- [ ] Building technical skills is strategic priority

**Premature Optimization:**
- [ ] Problem doesn't exist yet (0 errors, 0 users, 0 pageviews)
- [ ] Time spent on tooling > time spent on product
- [ ] Optimizing observability before validating product-market fit

**Verdict:** If 2+ boxes checked → DEFER tool adoption, use DIY

### When Tools ADD Value (Embrace Adoption)

Check for these pro-patterns:

**Volume Overwhelming:**
- [ ] >500 events/month (debugging is 40+ hrs/month)
- [ ] Missing critical events (errors slip through, incidents undetected)
- [ ] Can't keep up with manual inspection

**Team Coordination:**
- [ ] 5+ person team needs shared dashboards
- [ ] On-call rotation requires centralized alerting
- [ ] Cross-functional teams (eng + product + marketing) need access

**Pattern Detection:**
- [ ] Tool aggregation reveals systemic issues hidden in logs
- [ ] Correlation across events (error → slow query → user drop-off)
- [ ] Historical trend analysis informs architecture decisions

**Compliance/Security:**
- [ ] Customer contracts require SOC 2, BAA, ISO 27001
- [ ] GDPR compliance needs privacy-first tools
- [ ] Security team requires centralized audit logs

**Verdict:** If 2+ boxes checked → JUSTIFY tool adoption, proceed to S1

---

## 6. Decision Matrix Output

Based on analysis above, recommend one of three paths:

### Path 1: DIY (Build Yourself)
- [ ] **RECOMMENDED** for this category
- **Estimated effort:** [Hours]
- **When to revisit:** [Volume threshold or team size trigger]
- **Reference implementation:** [Link to code example]

### Path 2: Open Standard (Future-Proof)
- [ ] **RECOMMENDED** for this category
- **Standard:** [Name of standard]
- **Estimated effort:** [Hours]
- **Backend options:** [List 3-5 backends you can switch to]

### Path 3: Managed Tool (Operational Efficiency)
- [ ] **RECOMMENDED** for this category
- **Proceed to S1:** Research managed providers
- **Volume justification:** [Current volume vs threshold]
- **Team justification:** [Team size or coordination needs]

---

## 7. Summary

**Category:** [Name]
**Default recommendation:** Path [1/2/3]

**For Path 1 (DIY):**
- Setup time: [Hours]
- Monthly maintenance: [Hours]
- Volume ceiling: [Events/month before tools justified]

**For Path 2 (Open Standard):**
- Standard name: [Name]
- Setup time: [Hours]
- Backend options: [Count]
- Migration time: [Hours - should be <5 hours]

**For Path 3 (Managed):**
- Volume justification: [Current > threshold]
- Proceed to S1 provider search

**Tools NOT needed when:**
- [Condition 1: e.g., <100 errors/month]
- [Condition 2: e.g., Solo founder values learning]
- [Condition 3: e.g., Pre-PMF exploration]

**Tools JUSTIFIED when:**
- [Condition 1: e.g., >500 events/month]
- [Condition 2: e.g., Team coordination needed]
- [Condition 3: e.g., Compliance requirements]

---

**Next Steps:**
- If Path 1 (DIY): See DIY implementation guide [link]
- If Path 2 (Open Standard): See instrumentation guide [link]
- If Path 3 (Managed): Proceed to S1 Rapid Provider Search

---

**Research Time:** 30-45 minutes
**Value:** Saves 2-10 hours researching tools you don't need
