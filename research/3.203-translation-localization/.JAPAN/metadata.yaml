code: '3.203.JAPAN'
title: Japanese-Specific LLMs for Translation
tier: 3
category: Speech & Audio AI (Translation - Japanese Specialization)
status: s1_complete
started: 2025-11-26
s1_completed: 2025-11-26
estimated_hours: 4-6
actual_hours_s1: 1.5
s2_s3_s4_status: planned_not_started

rationale: |
  Japanese-specific LLM investigation for translation quality improvements.
  Hypothesis: Japanese-native LLMs (Rinna, LINE HyperCLOVA, CyberAgent CALM, Stability AI)
  might offer better tokenization efficiency, keigo handling, and cultural nuance understanding.

models_evaluated:
  open_source:
    - Rinna qwen2.5-bakeneko-32b (32B parameters, Japanese-optimized tokenizer)
    - CyberAgent CALM3-22B (22.5B parameters, Apache 2.0 license)
    - Stability AI Japanese StableLM Gamma 7B (7B parameters)
  commercial_restricted:
    - LINE HyperCLOVA X (Japan-only API, parameters undisclosed)

key_findings:
  - NO production-ready commercial APIs (all models are open-source, self-host only)
  - Western LLMs (GPT-4, Claude 3.5) already handle Japanese excellently (WMT24 winner)
  - Tokenization efficiency advantage is THEORETICAL (40-60% fewer tokens claimed, unverified)
  - Cultural nuance/keigo advantage is UNVERIFIED (no published benchmarks vs GPT-4/Claude)
  - Self-hosting only viable at massive scale (>1B chars/month = $12K-25K/month infrastructure)
  - Research gap: No Japanese LLM vs GPT-4/Claude translation benchmarks exist
  - HyperCLOVA X API restricted to Japan (not globally available)
  - CALM3 largest open-source Japanese LLM (22.5B parameters, commercially usable)

recommended_approach:
  commercial_apps: "Use Claude 3.5 Haiku ($4.80/M) or GPT-4o ($12.50/M) - proven quality, commercial API"
  high_volume: "Self-host Rinna or CALM3 if >1B chars/month (break-even at $12K-25K/month)"
  privacy_critical: "Self-host CALM3-22B (Apache 2.0, data stays on-premises)"
  research: "Conduct head-to-head benchmarks (Japanese LLM vs GPT-4 vs Claude for translation quality)"

s1_documents:
  - 00-SYNTHESIS.md (comprehensive analysis of Japanese LLM landscape)

s1_stats:
  documents: 1
  models: 4
  lines: 408
  size_kb: 24

next_steps:
  s2_comprehensive:
    - Translation benchmarks (BLEU scores, human evaluation for Japanese LLMs vs GPT-4/Claude)
    - Tokenization efficiency study (measure actual token counts for identical Japanese texts)
    - Self-hosting TCO analysis (GPU costs, ML engineering, maintenance)
    - Keigo handling accuracy (formal vs informal translation quality)
  s3_need_driven:
    - Use case #1: Japanese language learning app (API vs self-hosted)
    - Use case #2: High-volume translation service (>1B chars/month)
    - Use case #3: Privacy-critical Japanese translation (HIPAA, government)
    - Use case #4: Keigo-aware business translation
  s4_strategic:
    - Japanese LLM roadmap (will they become commercial APIs in 2026-2027?)
    - Western LLM Japanese improvements (GPT-5, Claude 4 trajectory)
    - Build vs buy decision framework (when to self-host vs use API)

trigger: User question about Japanese-tuned LLMs for cultural nuance (Nov 26, 2025)
parent_research: 3.203 Translation & Localization Services
related_research:
  - 3.203 Translation & Localization (main research, completed S1 Nov 26, 2025)
  - 1.105 Translation & i18n Libraries (Tier 1 self-hosted alternatives)

integration_relationships:
  upstream: "3.203 Translation (general translation platform comparison)"
  downstream: "Japanese language learning app (keigo-aware translation, cultural context)"
  adjacent: "3.204 TTS (Japanese voice synthesis), 3.205 Pronunciation (Japanese pronunciation assessment)"
