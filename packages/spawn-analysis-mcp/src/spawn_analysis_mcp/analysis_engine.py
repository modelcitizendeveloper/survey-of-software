"""
Analysis Engine - Business logic for conducting spawn-analysis.

Orchestrates parallel analyst execution with Bayesian confidence tracking.
Uses Claude's Task tool to spawn parallel analyst agents.
"""

import json
import re
from datetime import datetime
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, asdict

from spawn_analysis_mcp.analyst_registry import get_registry


@dataclass
class AnalysisResult:
    """Result from a single analyst."""

    analyst_id: str
    analyst_name: str
    analysis: str
    confidence: Optional[float] = None
    reasoning: Optional[str] = None
    timestamp: Optional[str] = None
    tokens_input: Optional[int] = None  # Input tokens consumed
    tokens_output: Optional[int] = None  # Output tokens generated

    def to_dict(self) -> dict:
        return asdict(self)


@dataclass
class AnalysisSession:
    """Complete analysis session with all analyst results."""

    session_id: str
    question: str
    context: str
    analysts: List[str]
    results: List[AnalysisResult]
    confidence_evolution: List[Tuple[str, float]]
    final_confidence: Optional[float] = None
    started_at: Optional[str] = None
    completed_at: Optional[str] = None
    status: str = "pending"  # pending, running, completed, failed
    user_id: Optional[str] = None  # User ownership for privacy
    token_usage: Optional[Dict[str, Dict[str, int]]] = None  # {analyst_id: {input: X, output: Y}}
    total_tokens: Optional[Dict[str, int]] = None  # {input: X, output: Y, total: Z}

    def to_dict(self) -> dict:
        d = asdict(self)
        d["results"] = [r.to_dict() for r in self.results]
        return d


class AnalysisEngine:
    """Engine for conducting spawn-analysis."""

    def __init__(self):
        """Initialize the analysis engine."""
        self.registry = get_registry()
        self._sessions: Dict[str, AnalysisSession] = {}

    def conduct_analysis(
        self,
        question: str,
        context: str,
        analyst_ids: Optional[List[str]] = None,
        prior_confidence: float = 0.5,
        user_id: Optional[str] = None,
        registry=None,
    ) -> AnalysisSession:
        """
        Conduct a spawn-analysis decision analysis.

        This is the core orchestration function. It:
        1. Validates analyst IDs
        2. Creates an analysis session
        3. Returns session for caller to execute analysts
           (Actual execution happens in MCP tool layer using Task tool)

        Args:
            question: The decision question
            context: Additional context for the decision
            analyst_ids: List of analyst IDs to use (None = all analysts)
            prior_confidence: Starting Bayesian confidence (0.0-1.0)
            user_id: User identifier for privacy/scoping (optional)
            registry: Optional analyst registry to use (overrides self.registry)

        Returns:
            AnalysisSession object (status: pending)

        Raises:
            ValueError: If analyst IDs are invalid
        """
        # Use provided registry or fall back to instance registry
        if registry is None:
            registry = self.registry

        # Validate analysts
        if analyst_ids is None:
            # Use all analysts in order
            analysts = registry.get_all_analysts()
            analyst_ids = [a.id for a in analysts]
        else:
            # Validate provided IDs
            for aid in analyst_ids:
                if not registry.get_analyst(aid):
                    raise ValueError(f"Unknown analyst: {aid}")

        # Generate session ID
        session_id = self._generate_session_id()

        # Create session
        session = AnalysisSession(
            session_id=session_id,
            question=question,
            context=context,
            analysts=analyst_ids,
            results=[],
            confidence_evolution=[(f"prior", prior_confidence)],
            started_at=datetime.now().isoformat(),
            status="pending",
            user_id=user_id,
            token_usage={},
            total_tokens={"input": 0, "output": 0, "total": 0},
        )

        # Store session
        self._sessions[session_id] = session

        return session

    def add_analyst_result(
        self,
        session_id: str,
        analyst_id: str,
        analysis: str,
        confidence: Optional[float] = None,
        tokens_input: Optional[int] = None,
        tokens_output: Optional[int] = None,
    ) -> AnalysisSession:
        """
        Add an analyst result to a session.

        Args:
            session_id: Session identifier
            analyst_id: Analyst identifier
            analysis: Analysis text from the analyst
            confidence: Optional confidence value (0.0-1.0).
                       If None, will try to extract from analysis text.
            tokens_input: Input tokens consumed by this analyst
            tokens_output: Output tokens generated by this analyst

        Returns:
            Updated AnalysisSession

        Raises:
            ValueError: If session not found
        """
        if session_id not in self._sessions:
            raise ValueError(f"Session not found: {session_id}")

        session = self._sessions[session_id]

        # Extract confidence if not provided
        if confidence is None:
            confidence = self.extract_confidence_from_text(analysis)

        # Get analyst info
        analyst = self.registry.get_analyst(analyst_id)
        analyst_name = analyst.name if analyst else analyst_id

        # Create result
        result = AnalysisResult(
            analyst_id=analyst_id,
            analyst_name=analyst_name,
            analysis=analysis,
            confidence=confidence,
            timestamp=datetime.now().isoformat(),
            tokens_input=tokens_input,
            tokens_output=tokens_output,
        )

        session.results.append(result)

        # Update confidence evolution if provided
        if confidence is not None:
            session.confidence_evolution.append((analyst_id, confidence))
            session.final_confidence = confidence

        # Update token usage tracking
        if tokens_input is not None or tokens_output is not None:
            session.token_usage[analyst_id] = {
                "input": tokens_input or 0,
                "output": tokens_output or 0,
            }
            # Update totals
            session.total_tokens["input"] += tokens_input or 0
            session.total_tokens["output"] += tokens_output or 0
            session.total_tokens["total"] = (
                session.total_tokens["input"] + session.total_tokens["output"]
            )

        # Update status
        if len(session.results) >= len(session.analysts):
            session.status = "completed"
            session.completed_at = datetime.now().isoformat()
        elif session.status == "pending":
            session.status = "running"

        return session

    def get_session(self, session_id: str) -> Optional[AnalysisSession]:
        """
        Get an analysis session by ID.

        Args:
            session_id: Session identifier

        Returns:
            AnalysisSession or None if not found
        """
        return self._sessions.get(session_id)

    def _generate_session_id(self) -> str:
        """Generate a unique session ID."""
        timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
        return f"analysis-{timestamp}"

    def extract_confidence_from_text(self, text: str) -> Optional[float]:
        """
        Extract confidence value from analyst output.

        Looks for patterns like:
        - "Confidence: 85%"
        - "Confidence: 0.85"
        - "Updated Confidence: 72%"

        Args:
            text: Analyst output text

        Returns:
            Confidence value (0.0-1.0) or None if not found
        """
        # Try percentage format first (85%)
        match = re.search(
            r"(?:updated\s+)?confidence:?\s*(\d+(?:\.\d+)?)\s*%",
            text,
            re.IGNORECASE,
        )
        if match:
            return float(match.group(1)) / 100.0

        # Try decimal format (0.85)
        match = re.search(
            r"(?:updated\s+)?confidence:?\s*(0?\.\d+)", text, re.IGNORECASE
        )
        if match:
            return float(match.group(1))

        return None


# Singleton instance
_engine: Optional[AnalysisEngine] = None


def get_engine() -> AnalysisEngine:
    """
    Get the singleton analysis engine instance.

    Returns:
        AnalysisEngine instance
    """
    global _engine
    if _engine is None:
        _engine = AnalysisEngine()
    return _engine
