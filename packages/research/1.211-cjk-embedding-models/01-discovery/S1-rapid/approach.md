# S1 Rapid Discovery: CJK Embedding Models

## Objective
Quick landscape survey of major embedding models with strong Chinese, Japanese, and Korean (CJK) language support.

## Methodology
- Identify 5 representative embedding models spanning different approaches
- Focus on architecture, CJK language support, and performance characteristics
- Document basic capabilities without deep technical dive
- Time box: Surface-level understanding to guide S2 deep dive

## Models Selected
1. **M3E** - Chinese-focused embedding model from Moka AI
2. **text2vec-chinese** - Chinese text vectorization library
3. **sentence-transformers** - Multilingual sentence embeddings (with CJK support)
4. **LaBSE** - Google's Language-agnostic BERT Sentence Embedding
5. **multilingual-e5** - Microsoft's multilingual embedding model (E5 family)

## Key Questions
- What languages are supported?
- How is CJK handled (tokenization, training data)?
- What are typical embedding dimensions?
- Open-source vs commercial?
- Performance on CJK semantic similarity tasks?

## Pass Criteria
- Individual model profiles complete
- Basic architecture understanding documented
- Language support clearly identified
- Recommendation for S2 focus areas
