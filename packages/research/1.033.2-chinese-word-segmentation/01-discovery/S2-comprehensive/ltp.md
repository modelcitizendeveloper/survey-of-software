# LTP: Deep Technical Analysis

**Experiment**: 1.033.2 Chinese Word Segmentation Libraries
**Pass**: S2 - Comprehensive Discovery
**Date**: 2026-01-28

## Algorithm & Architecture

### Core Algorithm: Multi-Task Knowledge Distillation

LTP employs a sophisticated architecture combining multi-task learning with knowledge distillation:

#### Neural Architecture (Deep Learning Models)

**1. Shared Encoder**
- **Base model**: BERT-based transformer (Chinese)
- Pre-trained: Large-scale Chinese corpora (Wikipedia, news, web)
- Hidden size: 768 (Base), 512 (Small), 256 (Tiny)
- Layers: 12 (Base), 6 (Small), 3 (Tiny)

**2. Task-Specific Decoders**

Each NLP task has dedicated output layer:

| Task | Decoder Architecture | Output |
|------|---------------------|--------|
| **Word Segmentation (CWS)** | BiLSTM-CRF | BIO tags |
| **Part-of-Speech (POS)** | BiLSTM-Softmax | POS labels |
| **Named Entity Recognition (NER)** | BiLSTM-CRF | Entity tags |
| **Dependency Parsing (DP)** | Biaffine Attention | Dependency arcs |
| **Semantic Dependency Parsing (SDP)** | Biaffine Attention | Semantic arcs |
| **Semantic Role Labeling (SRL)** | BiLSTM-CRF | Argument labels |

**3. Multi-Task Learning Framework**

```
Input Text ‚Üí BERT Encoder (shared) ‚Üí Task-Specific Decoders ‚Üí Outputs
                      ‚Üì
            [CWS] [POS] [NER] [DP] [SDP] [SRL]
```

**Benefits**:
- Shared representations improve generalization
- Joint training captures task correlations
- Single model serves multiple purposes

#### Knowledge Distillation Technique

**Two-stage training**:

**Stage 1: Single-Task Teachers**
- Train 6 separate models (one per task)
- Each optimized independently
- Achieve task-specific state-of-the-art

**Stage 2: Multi-Task Student**
- Single model learns from all teachers
- Distillation loss: Minimize divergence from teacher predictions
- Preserves accuracy while reducing model count

**Mathematical formulation**:
```
Loss = Œ± * L_task + (1-Œ±) * L_distill
L_distill = KL(P_student || P_teacher)
```

**Advantage**: 6 models ‚Üí 1 model with minimal accuracy loss

### Legacy Architecture (Rust Implementation)

**Algorithm**: Structured Perceptron
- Faster than neural models (no deep layers)
- Feature-based (similar to CRF)
- Rust implementation for speed
- Limited to 3 tasks: CWS, POS, NER

**Speed comparison**:
- Legacy: 21,581 sentences/second (16 threads)
- Deep learning: 39-53 sentences/second
- **500x faster** for basic tasks

### Segmentation Approach: Character Tagging

Like CKIP and PKUSeg, LTP uses BIO sequence labeling:

```
Input:  ‰ªñ Âè´ Ê±§ ÂßÜ Âéª Êãø Â§ñ Ë°£ „ÄÇ
Tags:   S  S  B  I  S  S  B  I  S
Output: [‰ªñ] [Âè´] [Ê±§ÂßÜ] [Âéª] [Êãø] [Â§ñË°£] [„ÄÇ]
```

**Tag set**:
- **B**: Begin word
- **I**: Inside word
- **S**: Single-character word

**BERT enhancement**: Contextual embeddings capture semantic nuances

### Unknown Word Handling

**Subword tokenization** (BERT):
- Splits unknown words into known subwords
- Example: "ChatGPT" ‚Üí ["Chat", "##G", "##P", "##T"]
- No true OOV problem at character level

**Character-level features**:
- BERT processes every character
- Learns morphological patterns
- Strong performance on:
  - Person names (Âº†‰ºü, ÊùéÊòé)
  - Organization names (ÈòøÈáåÂ∑¥Â∑¥, Â≠óËäÇË∑≥Âä®)
  - Neologisms (ÁΩëÁ∫¢, ÊâìÂç°)

## Performance Deep Dive

### Model Size Comparison

| Model | Parameters | Size | Speed (sent/s) | CWS Accuracy |
|-------|-----------|------|----------------|--------------|
| **Base** | 110M | 500 MB | 39 | **98.7%** |
| **Base1** | 110M | 500 MB | 39 | 98.5% |
| **Base2** | 110M | 500 MB | 39 | 98.6% |
| **Small** | 60M | 250 MB | 43 | **98.4%** |
| **Tiny** | 25M | 100 MB | 53 | 96.8% |
| **Legacy** | ‚Äî | 50 MB | 21,581 | ~95%* |

*Legacy accuracy estimated based on LTP v3 benchmarks

### CPU vs GPU Requirements

**CPU Inference** (Intel Xeon E5-2680 v4):

| Model | CPU Speed | Memory |
|-------|-----------|--------|
| Base | 5-10 sent/s | 2 GB |
| Small | 8-15 sent/s | 1.5 GB |
| Tiny | 12-20 sent/s | 1 GB |
| Legacy | 1,300 sent/s (single-thread) | 512 MB |

**GPU Inference** (NVIDIA V100):

| Model | GPU Speed | VRAM |
|-------|-----------|------|
| Base | 100-150 sent/s | 2 GB |
| Small | 150-200 sent/s | 1.5 GB |
| Tiny | 200-250 sent/s | 1 GB |

**Recommendation**:
- CPU: Use Legacy or Tiny for production
- GPU: Use Base or Small for best accuracy

### Memory Footprint

**Deep Learning Models**:

| Component | Base | Small | Tiny |
|-----------|------|-------|------|
| Model weights | 500 MB | 250 MB | 100 MB |
| BERT embeddings | 300 MB | 150 MB | 80 MB |
| Runtime memory (CPU) | 2 GB | 1.5 GB | 1 GB |
| Runtime memory (GPU) | 2 GB VRAM | 1.5 GB VRAM | 1 GB VRAM |

**Legacy Model**:
- Model weights: 50 MB
- Runtime memory: 512 MB
- No GPU required

### Benchmark Results

#### LTP Internal Benchmarks (Accuracy %)

| Model | CWS | POS | NER | SRL | DP | SDP |
|-------|-----|-----|-----|-----|----|----|
| **Base** | **98.7** | **98.5** | 95.4 | 80.6 | 89.5 | 75.2 |
| **Small** | **98.4** | **98.2** | 94.3 | 78.4 | 88.3 | 74.7 |
| **Tiny** | 96.8 | 97.1 | 91.6 | 70.9 | 83.8 | 70.1 |

**Test datasets**: CTB, OntoNotes, SemEval

#### Comparative Benchmarks (PKU Dataset)

| Tool | F1 Score |
|------|----------|
| PKUSeg | 95.4% |
| THULAC | 92.4% |
| LTP | 88.7% |
| Jieba | 81.2% |

**Note**: LTP Base achieves 98.7% on its benchmarks but 88.7% on PKU dataset
- **Reason**: Different evaluation protocols and datasets
- LTP optimized for multi-task performance, not single-task segmentation

### Latency Characteristics

**Single sentence** (30 characters):

| Model | CPU | GPU |
|-------|-----|-----|
| Base | 200-300ms | 20-30ms |
| Small | 150-200ms | 15-20ms |
| Tiny | 100-150ms | 10-15ms |
| Legacy | 1-2ms | N/A |

**Batch processing** (100 sentences):

| Model | CPU | GPU |
|-------|-----|-----|
| Base | 10-20s | 1-2s |
| Small | 6-12s | 0.5-1s |
| Tiny | 5-8s | 0.4-0.6s |
| Legacy | 0.1s (16 threads) | N/A |

**Optimization**: Batch processing critical for GPU efficiency

### Scalability Characteristics

**Single-threaded** (CPU):
- Base: ~10 sent/s
- Legacy: ~1,300 sent/s

**Multi-threaded** (CPU, 16 cores):
- Base: ~40 sent/s (4x speedup, diminishing returns)
- Legacy: ~21,581 sent/s (17x speedup, near-linear)

**Multi-GPU** (experimental):
- Data parallelism: Linear scaling up to 4 GPUs
- Model parallelism: Not yet implemented

## Deployment Requirements

### Dependencies

**Deep Learning Models**:
```
torch>=1.11.0
transformers>=4.20.0
pygtrie
```

**Legacy Model**:
```
ltp-core>=0.1.0  # Rust bindings
```

**Installation**:
```bash
# Standard (deep learning)
pip install ltp

# Legacy only (lightweight)
pip install ltp-core
```

### Platform Support

| Platform | Deep Learning | Legacy |
|----------|--------------|--------|
| Linux | ‚úÖ Full | ‚úÖ Full |
| macOS | ‚úÖ Full | ‚úÖ Full |
| Windows | ‚úÖ Full | ‚úÖ Full |
| Docker | ‚úÖ Full | ‚úÖ Full |
| ARM (Raspberry Pi) | ‚ö†Ô∏è CPU only | ‚úÖ Full |

### Python Versions

- **Python 3.6+**: Required
- **Python 2.x**: Not supported
- Tested: 3.7, 3.8, 3.9, 3.10, 3.11

### Disk Space Requirements

| Component | Size | Required? |
|-----------|------|-----------|
| ltp package | 30 MB | ‚úÖ Yes |
| Base model | 500 MB | ‚ùå Optional |
| Small model | 250 MB | ‚ùå Optional |
| Tiny model | 100 MB | ‚ùå Optional |
| Legacy model | 50 MB | ‚ùå Optional |
| **Typical (Small)** | **~280 MB** | ‚Äî |

### Network Requirements

**Initial setup**: Internet for model download
- Source: Hugging Face Hub
- Mirrors: Tsinghua, Alibaba Cloud (China)
- Size: 100-500 MB per model

**Production**: No internet required (models cached locally)

**Offline deployment**:
```python
from ltp import LTP
ltp = LTP("/path/to/local/model")
```

### GPU Requirements (Optional)

**Minimum**:
- CUDA 10.2+
- cuDNN 7.6+
- 2 GB VRAM

**Recommended**:
- CUDA 11.8+
- cuDNN 8.6+
- 4-8 GB VRAM (for batch processing)

**Installation**:
```bash
pip install ltp torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

## Integration Patterns

### Basic API

```python
from ltp import LTP

# Initialize (auto-downloads from Hugging Face)
ltp = LTP("LTP/small")  # or "LTP/base", "LTP/tiny"

# Move to GPU (optional)
ltp.to("cuda")

# Segment text
sentences = ["‰ªñÂè´Ê±§ÂßÜÂéªÊãøÂ§ñË°£„ÄÇ", "ÊàëÁà±Âåó‰∫¨Â§©ÂÆâÈó®„ÄÇ"]
output = ltp.pipeline(sentences, tasks=["cws"])
print(output.cws)
# [['‰ªñ', 'Âè´', 'Ê±§ÂßÜ', 'Âéª', 'Êãø', 'Â§ñË°£', '„ÄÇ'],
#  ['Êàë', 'Áà±', 'Âåó‰∫¨', 'Â§©ÂÆâÈó®', '„ÄÇ']]
```

### Multi-Task Processing

```python
from ltp import LTP

ltp = LTP("LTP/small")

sentences = ["‰ªñÂè´Ê±§ÂßÜÂéªÊãøÂ§ñË°£„ÄÇ"]

# Run multiple tasks in single pass
output = ltp.pipeline(
    sentences,
    tasks=["cws", "pos", "ner", "dep", "sdp", "srl"]
)

# Word segmentation
print(output.cws)
# [['‰ªñ', 'Âè´', 'Ê±§ÂßÜ', 'Âéª', 'Êãø', 'Â§ñË°£', '„ÄÇ']]

# POS tagging
print(output.pos)
# [['r', 'v', 'nh', 'v', 'v', 'n', 'wp']]

# Named entities
print(output.ner)
# [(2, 3, 'Nh', 'Ê±§ÂßÜ')]  # (start, end, type, text)

# Dependency parsing
print(output.dep)
# [(2, 1, 'SBV'), (2, 4, 'COO'), ...]  # (head, index, relation)

# Semantic role labeling
print(output.srl)
# [[(1, 0, 1, 'A0'), ...]]  # (predicate_pos, start, end, role)
```

**Efficiency**: Single forward pass for all tasks (shared encoder)

### Batch Processing

```python
from ltp import LTP

ltp = LTP("LTP/small")
ltp.to("cuda")

# Process large corpus
def segment_file(input_path, output_path, batch_size=32):
    sentences = []
    results = []

    with open(input_path, 'r', encoding='utf-8') as f:
        for line in f:
            sentences.append(line.strip())
            if len(sentences) >= batch_size:
                output = ltp.pipeline(sentences, tasks=["cws"])
                results.extend(output.cws)
                sentences = []

        # Process remaining
        if sentences:
            output = ltp.pipeline(sentences, tasks=["cws"])
            results.extend(output.cws)

    # Write output
    with open(output_path, 'w', encoding='utf-8') as f:
        for seg_list in results:
            f.write(" ".join(seg_list) + "\n")

segment_file("input.txt", "output.txt", batch_size=64)
```

**Optimization**: Batch size 32-64 optimal for GPU

### Legacy Model Integration

```python
from ltp import LTP

# Use legacy model (fast, CPU-only)
ltp = LTP("LTP/legacy")

sentences = ["‰ªñÂè´Ê±§ÂßÜÂéªÊãøÂ§ñË°£„ÄÇ"]

# Only supports CWS, POS, NER
output = ltp.pipeline(sentences, tasks=["cws", "pos", "ner"])

print(output.cws)
# [['‰ªñ', 'Âè´', 'Ê±§ÂßÜ', 'Âéª', 'Êãø', 'Â§ñË°£', '„ÄÇ']]
```

**Use case**: High-throughput production (21K sent/s)

### Custom Dictionary (Experimental)

**Note**: LTP doesn't officially support custom dictionaries like Jieba/PKUSeg
**Workaround**: Pre-processing or post-processing

```python
import ltp
from ltp import LTP

# Pre-processing: Replace known terms with placeholders
def preprocess(text, custom_dict):
    for term in custom_dict:
        text = text.replace(term, f"<TERM{hash(term)}>")
    return text

# Post-processing: Restore original terms
def postprocess(segments, custom_dict):
    # Restore placeholders
    # ...
    return segments

# Usage
ltp_model = LTP("LTP/small")
text = preprocess("ÊàëÂú®ÈòøÈáåÂ∑¥Â∑¥Â∑•‰Ωú", ["ÈòøÈáåÂ∑¥Â∑¥"])
output = ltp_model.pipeline([text], tasks=["cws"])
result = postprocess(output.cws[0], ["ÈòøÈáåÂ∑¥Â∑¥"])
```

**Better approach**: Fine-tune model on domain data

### Streaming Processing

```python
from ltp import LTP

ltp = LTP("LTP/small")
ltp.to("cuda")

def process_stream(file_path, batch_size=32):
    batch = []

    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            batch.append(line.strip())
            if len(batch) >= batch_size:
                output = ltp.pipeline(batch, tasks=["cws"])
                yield from output.cws
                batch = []

        # Process remaining
        if batch:
            output = ltp.pipeline(batch, tasks=["cws"])
            yield from output.cws

# Usage
for segmented_line in process_stream("large_file.txt"):
    process(segmented_line)
```

## Architecture Strengths

### Design Philosophy
1. **Multi-task learning**: Shared knowledge across NLP tasks
2. **Flexible accuracy/speed**: Multiple model sizes
3. **Research-driven**: Based on latest NLP advances (EMNLP 2021)
4. **Production-ready**: Legacy model for high-throughput

### Multi-Task Advantages

**vs. Single-Task Tools**:
- ‚úÖ One model for 6 tasks (reduced deployment complexity)
- ‚úÖ Shared representations (better generalization)
- ‚úÖ Consistent preprocessing (no multi-tool integration issues)
- ‚úÖ End-to-end NLP pipeline in single API call

**Example use case**: Document analysis
```python
# Single call for complete NLP analysis
output = ltp.pipeline(doc, tasks=["cws", "pos", "ner", "dep", "srl"])
# Extract: entities, dependencies, semantic roles
```

### Knowledge Distillation Benefits

**6 models ‚Üí 1 model**:
- üóúÔ∏è **Model compression**: 3 GB ‚Üí 500 MB
- ‚ö° **Faster inference**: 6 calls ‚Üí 1 call
- üéØ **Preserved accuracy**: ~98% of teacher performance
- üíæ **Reduced deployment**: Single artifact

### Speed/Accuracy Tradeoff

**Flexible model selection**:

| Scenario | Model Choice | Rationale |
|----------|--------------|-----------|
| Research, max accuracy | Base | 98.7% CWS, 80.6% SRL |
| Production, balanced | Small | 98.4% CWS, 43 sent/s |
| Real-time, low latency | Tiny | 96.8% CWS, 53 sent/s |
| High-throughput batch | Legacy | ~95% CWS, 21K sent/s |

**Unique advantage**: Single toolkit, multiple performance profiles

## When LTP Excels

‚úÖ **Optimal for**:
- Multi-task NLP pipelines (WS + POS + NER + parsing + SRL)
- Research applications requiring semantic analysis
- Flexible deployment (choose model size for environment)
- Enterprise systems needing institutional backing (HIT, Baidu, Tencent)
- High-throughput batch processing (Legacy model)
- Applications requiring both speed and accuracy options

‚ö†Ô∏è **Limitations**:
- Single-task segmentation (PKUSeg/CKIP more accurate)
- Model size (larger than single-task tools)
- Commercial licensing (requires agreement with HIT)
- Limited custom dictionary support (compared to Jieba)

## Production Deployment Patterns

### Docker Deployment (GPU)

```dockerfile
FROM nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04
RUN apt-get update && apt-get install -y python3-pip
RUN pip install ltp torch

# Pre-download models during build
RUN python3 -c "from ltp import LTP; LTP('LTP/small')"

COPY app.py /app/
WORKDIR /app
CMD ["python3", "app.py"]
```

**Image size**: ~3 GB (CUDA + PyTorch + LTP Small)

### Docker Deployment (CPU, Legacy)

```dockerfile
FROM python:3.10-slim
RUN pip install ltp

# Download legacy model
RUN python3 -c "from ltp import LTP; LTP('LTP/legacy')"

COPY app.py /app/
WORKDIR /app
CMD ["python3", "app.py"]
```

**Image size**: ~300 MB (Python + LTP Legacy)

### API Wrapper (FastAPI)

```python
from fastapi import FastAPI
from pydantic import BaseModel
from ltp import LTP

app = FastAPI()

# Preload model
ltp = LTP("LTP/small")
ltp.to("cuda")  # Or "cpu"

class PipelineRequest(BaseModel):
    sentences: list[str]
    tasks: list[str] = ["cws"]

@app.post("/pipeline")
def pipeline(request: PipelineRequest):
    output = ltp.pipeline(request.sentences, tasks=request.tasks)
    return {
        "cws": output.cws if "cws" in request.tasks else None,
        "pos": output.pos if "pos" in request.tasks else None,
        "ner": output.ner if "ner" in request.tasks else None,
        "dep": output.dep if "dep" in request.tasks else None,
        "srl": output.srl if "srl" in request.tasks else None,
    }
```

**Throughput**:
- GPU (Small): 100-200 req/s
- CPU (Small): 10-20 req/s
- CPU (Legacy): 1000+ req/s

### Kubernetes Deployment

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ltp-service
spec:
  replicas: 3
  template:
    spec:
      containers:
      - name: ltp
        image: ltp-service:latest
        resources:
          limits:
            nvidia.com/gpu: 1
            memory: 4Gi
        env:
        - name: LTP_MODEL
          value: "LTP/small"
```

**Scaling strategies**:
- GPU pods: Vertical scaling (larger GPU)
- CPU pods: Horizontal scaling (more replicas)
- Hybrid: GPU for accuracy-critical, Legacy for high-volume

### Serverless Considerations

**Challenges**:
- Cold start: 5-15s (model loading)
- Model size: 100-500 MB (manageable for serverless)
- GPU: Limited availability

**Strategies**:
- Use Tiny model (100 MB, fastest cold start)
- Pre-warm containers (provisioned concurrency)
- Cached models (EFS/Cloud Storage mount)

**Recommendation**: LTP less suitable for serverless vs. Jieba (smaller, faster load)

## Advanced Topics

### Fine-Tuning for Domain Adaptation

**Scenario**: Adapt LTP to legal domain

```python
from ltp import LTP
from transformers import Trainer, TrainingArguments

# Load base model
ltp = LTP("LTP/small")

# Prepare training data (BIO format)
train_dataset = load_legal_corpus()

# Fine-tune
training_args = TrainingArguments(
    output_dir="./ltp-legal",
    num_train_epochs=3,
    per_device_train_batch_size=16,
    learning_rate=5e-5,
)

trainer = Trainer(
    model=ltp.model,
    args=training_args,
    train_dataset=train_dataset,
)

trainer.train()
```

**Typical improvements**: 2-5% on domain-specific tasks

### Multi-Language Support (Future)

**Current**: Chinese only
**Roadmap**: English, other Asian languages

**Architecture**: Language-agnostic (BERT-based)
- Train language-specific encoders
- Share task-specific decoder architecture

### Integration with Downstream Tasks

**Example**: Sentiment analysis pipeline

```python
from ltp import LTP
import torch.nn as nn

ltp = LTP("LTP/small")

def sentiment_pipeline(text):
    # Step 1: LTP preprocessing
    output = ltp.pipeline([text], tasks=["cws", "pos", "ner"])

    # Step 2: Feature extraction
    words = output.cws[0]
    pos_tags = output.pos[0]
    entities = output.ner[0]

    # Step 3: Sentiment classifier (custom)
    sentiment = sentiment_classifier(words, pos_tags, entities)
    return sentiment

# Use LTP as feature extractor for ML models
```

## Licensing Considerations

**Apache 2.0 License**:
- ‚úÖ Free for academic/research use
- ‚ö†Ô∏è Commercial use requires licensing from HIT
- Contact: ltp-contact@hit.edu.cn

**Commercial licensing**:
- Pricing: Variable (contact HIT)
- Includes: Enterprise support, SLA, custom models
- Typical customers: Baidu, Tencent, Alibaba

**Alternatives for free commercial use**:
- Jieba: MIT (fully free)
- PKUSeg: MIT (fully free)
- CKIP: GNU GPL v3.0 (copyleft, derivatives must be GPL)

## References

- [GitHub Repository](https://github.com/HIT-SCIR/ltp)
- [PyPI Package](https://pypi.org/project/ltp/)
- [EMNLP 2021 Paper: N-LTP](https://aclanthology.org/2021.emnlp-demo.6/)
- [ArXiv Paper](https://arxiv.org/abs/2009.11616)
- [Hugging Face Models](https://huggingface.co/LTP)
- [LTP Cloud Service](https://www.ltp-cloud.com/intro_en)

## Cross-References

- **S1 Rapid Discovery**: [ltp.md](../S1-rapid/ltp.md) - Overview and quick comparison
- **S3 Need-Driven**: Multi-task NLP use cases (to be created)
- **S4 Strategic**: HIT backing and commercial licensing (to be created)
