# S1 Rapid Discovery: Multilingual & CJK LLMs

## Objective
Quick landscape survey of major multilingual language models with focus on Chinese, Japanese, and Korean (CJK) language support.

## Methodology
- Identify 5 representative models spanning different architectures and approaches
- Focus on pre-training approach, language coverage, and CJK performance claims
- Document basic capabilities without deep technical dive
- Time box: Surface-level understanding to guide S2 deep dive

## Models Selected
1. **BLOOM** - Multilingual open-source model (176B)
2. **XLM-RoBERTa** - Cross-lingual understanding via MLM
3. **mBERT** - Google's multilingual BERT baseline
4. **ERNIE** - Baidu's enhanced representation (strong Chinese focus)
5. **GPT-4 Multilingual** - Commercial state-of-the-art

## Key Questions
- What languages are supported?
- How is CJK handled (tokenization, training data)?
- What are the primary use cases?
- Open-source vs commercial?

## Pass Criteria
- Individual model profiles complete
- Basic architecture understanding documented
- Language support clearly identified
- Recommendation for S2 focus areas
