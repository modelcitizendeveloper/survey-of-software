# DeepEval

## Overview
- **Type**: Open-source Python framework
- **License**: Apache 2.0
- **GitHub**: 400k+ monthly downloads
- **Focus**: Comprehensive LLM evaluation ("Pytest for LLMs")

## Key Features
- **60+ metrics**: Prompt, RAG, chatbot, safety, multimodal
- **Self-explaining metrics**: Tells you WHY scores are low
- **Pytest integration**: Familiar unit-test interface
- **CI/CD native**: Built for continuous deployment workflows
- **Safety testing**: Red teaming, toxicity detection

## Metric Categories
- **RAG**: Faithfulness, contextual relevancy, answer relevancy
- **Conversational**: Coherence, engagement, knowledge retention
- **Safety**: Bias, toxicity, PII leakage, jailbreak detection
- **Agentic**: Tool use, task completion, reasoning

## Enterprise Platform (Confident AI)
- Cloud dashboard for team collaboration
- Dataset curation and annotation
- Production monitoring
- Regression detection

## Limitations
- Python-only (no JS/CLI-first option)
- Enterprise features require Confident AI platform
- Can be overkill for simple prompt testing

## Best For
- Teams needing comprehensive evaluation coverage
- CI/CD integration with automated testing
- Production monitoring and regression detection
- Multi-pattern evaluation (RAG, agents, chatbots)

## Installation
```bash
pip install deepeval
```

## Pricing
- **Open-source**: Free
- **Confident AI**: Free tier + paid plans for enterprise
