# Recent Advances (2025)

## Fine-tuned Large Language Models

A 2025 RANLP paper investigated Chinese dependency parsing using fine-tuned LLMs, specifically exploring how different dependency representations impact parsing performance when fine-tuning Chinese Llama-3.

### Key Findings
- Stanford typed dependency tuple representation yields highest number of valid dependency trees
- Converting dependency structure into lexical centered tree produces parses of significantly higher quality

## LLM-Assisted Data Augmentation

Research on Chinese dialogue-level dependency parsing shows LLMs can assist with data augmentation to improve parser training.

## Sources
- [Branching Out: Exploration of Chinese Dependency Parsing with Fine-tuned LLMs](https://acl-bg.org/proceedings/2025/RANLP%202025/pdf/2025.ranlp-1.166.pdf)
- [ACL Anthology](https://aclanthology.org/2025.ranlp-1.166/)
- [LLM-Assisted Data Augmentation for Chinese Dialogue-Level Dependency Parsing](https://direct.mit.edu/coli/article/50/3/867/120014/LLM-Assisted-Data-Augmentation-for-Chinese)
