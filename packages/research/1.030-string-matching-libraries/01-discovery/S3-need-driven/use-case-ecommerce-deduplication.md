# Use Case: E-Commerce Product Deduplication

## Who Needs This

**Persona**: Data Engineering Team at Growing E-Commerce Marketplace
- **Company**: Multi-vendor marketplace (think Etsy, Amazon Marketplace model)
- **Team Size**: 2-3 data engineers
- **Scale**: 5M products, 10K new listings daily
- **Industry**: General e-commerce (electronics, fashion, home goods)

## Why This Matters

**Business Problem:**
- Vendors list same products with slight title variations
- "iPhone 15 Pro 256GB Blue" vs "Apple iPhone 15Pro 256 GB - Blue Color"
- Duplicate listings:
  - Confuse buyers (which to choose?)
  - Dilute SEO (Google penalizes duplicates)
  - Reduce conversion (decision paralysis)
  - Waste vendor resources (competing against themselves)

**Pain Point:**
Current manual review process cannot scale:
- Reviewing 10K daily listings ‚Üí 500 staff hours/week
- High false positive rate (mark unique items as duplicates)
- High false negative rate (miss obvious duplicates)

**Goal:**
Automate duplicate detection to flag 80% of duplicates with < 5% false positive rate.

## Requirements

### Must-Have Features

‚úÖ **High throughput** - Process 10K listings/day (sustained), 50K/day (peak)
‚úÖ **Accuracy** - 80% recall (catch duplicates), 95% precision (few false positives)
‚úÖ **Fuzzy matching** - Handle typos, abbreviations, reordering
‚úÖ **Language support** - English, Spanish, French (international marketplace)
‚úÖ **Batch processing** - Compare new listings against 5M existing products

### Nice-to-Have Features

‚ö™ **Real-time API** - Warn vendor during listing creation
‚ö™ **Confidence scores** - Show similarity percentage to reviewers
‚ö™ **Token matching** - "Blue iPhone 15" = "iPhone 15 Blue" (word order)

### Constraints

üìä **Scale:** 10K new √ó 5M existing = 50 billion potential comparisons daily
‚è±Ô∏è **Latency:** Batch job can run overnight (8 hours OK)
üí∞ **Budget:** Limited - growing startup, cost-sensitive
üõ†Ô∏è **Team:** 2-3 engineers, not NLP experts
üîí **Accuracy:** 80% recall critical (missed duplicates hurt UX)

### Success Criteria

- Detect 80% of duplicates (current: 40% via manual review)
- < 5% false positive rate (don't block legitimate listings)
- Process 10K listings in < 8 hours
- < $500/month infrastructure cost

---

## Library Evaluation

### RapidFuzz - Fit Analysis

**Must-Haves:**
- ‚úÖ **High throughput**: 1,800 pairs/sec = 6.48M pairs/hour (sufficient for 50B in ~7,700 hours ‚ùå)
- ‚ö†Ô∏è **Needs optimization**: Can't compare every new listing to all 5M products
- ‚úÖ **Fuzzy matching**: Token sort ratio handles "Blue iPhone" = "iPhone Blue"
- ‚úÖ **Accuracy**: Configurable thresholds (tune recall vs precision)
- ‚úÖ **Language support**: Works with any Unicode text

**Nice-to-Haves:**
- ‚úÖ **Confidence scores**: Built-in (returns 0-100 similarity score)
- ‚úÖ **Token matching**: token_sort_ratio, token_set_ratio
- ‚ö†Ô∏è **Real-time**: Fast enough (< 1ms per comparison) but needs index

**Constraints:**
- üìä **Scale**: Needs blocking strategy (can't do 50B comparisons)
  - Solution: Block by category, brand, price range
  - Reduces comparisons to ~100K per listing (feasible!)
- ‚è±Ô∏è **Latency**: 100K √ó (1/1800) = 56 seconds per listing √ó 10K = 156 hours ‚ùå
  - **Fix**: Parallel processing (10 workers ‚Üí 15.6 hours ‚úÖ)
- üí∞ **Budget**: Memory-intensive (20-200 MB), but manageable
- üõ†Ô∏è **Team**: Simple API, minimal learning curve

**Fit Score:** 85/100

**Implementation Strategy:**
1. Block new listings by category + brand (reduce search space to ~100-1000 products)
2. Use token_sort_ratio for title comparison (handles word reordering)
3. Threshold tuning: similarity > 90 = likely duplicate
4. Parallel processing: 10 workers to meet 8-hour deadline

---

### Jellyfish - Fit Analysis

**Must-Haves:**
- ‚ùå **High throughput**: Slower than RapidFuzz
- ‚ö™ **Fuzzy matching**: Has Levenshtein, but no token-based matching
- ‚ö™ **Accuracy**: Distance metrics less intuitive than similarity scores
- ‚úÖ **Language support**: Works with Unicode

**Constraints:**
- ‚è±Ô∏è **Latency**: Slower than RapidFuzz ‚Üí won't meet 8-hour deadline
- üõ†Ô∏è **Team**: Limited token support ‚Üí would need custom code

**Fit Score:** 40/100

**Why Not Recommended:**
- Phonetic matching (Soundex) not useful for product titles
- Slower than RapidFuzz with no compensating advantages
- Lacks token-based matching (critical for word reordering)

---

### pyahocorasick - Fit Analysis

**Must-Haves:**
- ‚ùå **Fuzzy matching**: Only exact matching (not suitable)

**Fit Score:** 10/100

**Why Not Recommended:**
Product titles have too much variation for exact matching. "iPhone 15 Pro" ‚â† "iPhone 15 Pro Max" (exact match fails, but fuzzy match catches similarity).

---

## Comparison Matrix

| Requirement | RapidFuzz | Jellyfish | pyahocorasick |
|-------------|-----------|-----------|---------------|
| **Throughput (pairs/sec)** | 1,800 ‚úÖ | < 1,800 ‚ö†Ô∏è | N/A |
| **Fuzzy matching** | ‚úÖ‚úÖ Token-based | ‚úÖ Distance only | ‚ùå Exact only |
| **Accuracy** | ‚úÖ Tunable | ‚ö†Ô∏è Manual tuning | ‚ùå |
| **Latency (10K batch)** | 15.6h (10 workers) ‚úÖ | > 20h ‚ùå | N/A |
| **Token matching** | ‚úÖ Built-in | ‚ùå | ‚ùå |
| **Memory** | 20-200 MB ‚ö†Ô∏è | Higher ‚ö†Ô∏è | N/A |

---

## Recommendation

### Primary: **RapidFuzz**

**Fit: 85/100**

**Rationale:**

1. **Token-based matching is critical**: Product titles vary in word order
   - "Blue iPhone 15" vs "iPhone 15 Blue"
   - RapidFuzz's token_sort_ratio handles this natively
   - Jellyfish would require custom tokenization code

2. **Speed enables scale**: 1,800 pairs/sec sufficient with blocking strategy
   - Block by category + brand ‚Üí 100-1000 candidates per listing
   - Parallel processing (10 workers) ‚Üí meet 8-hour SLA

3. **Tunable accuracy**: Similarity scores (0-100) intuitive for threshold tuning
   - Start with 90% threshold
   - Measure precision/recall on validation set
   - Adjust threshold to meet 80% recall, < 5% FPR

4. **Production-proven**: 83M monthly downloads indicate reliability

**Implementation Approach:**

```python
# Conceptual approach (not full implementation)
from rapidfuzz import fuzz

def find_duplicates(new_listing, candidates):
    """
    new_listing: New product title
    candidates: List of existing product titles in same category/brand
    """
    scores = [(title, fuzz.token_sort_ratio(new_listing, title))
              for title in candidates]

    # Filter by threshold
    duplicates = [title for title, score in scores if score > 90]
    return duplicates

# Blocking strategy
def get_candidates(new_listing):
    # Reduce 5M products to ~100-1000 based on:
    # - Same category
    # - Same brand (if available)
    # - Price within 20% range
    pass
```

**Cost Estimate:**
- Compute: 10 workers √ó 8 hours √ó $0.10/hour = $8/day = $240/month ‚úÖ Under budget
- Memory: 200 MB √ó 10 workers = 2 GB total (minimal cost)

---

### Alternative: **Elasticsearch with fuzzy query** (not a library, but worth mentioning)

**When to consider:**
- If search infrastructure already exists
- Need real-time duplicate detection (during listing creation)
- Can afford managed service ($100-500/month)

**Trade-off:** Higher cost, but lower engineering effort (no custom blocking needed)

---

## Key Insights

**S3 reveals RapidFuzz's strength**: Token-based matching (token_sort_ratio) is essential for product title deduplication. This wasn't obvious in S1 (popularity) or S2 (algorithms) but becomes clear when mapping to real use case.

**Blocking strategy is critical**: Even fastest library can't do 50 billion comparisons. Success requires smart indexing (category, brand, price range) to reduce search space.

**False positives hurt**: 5% FPR on 10K listings = 500 false flags daily = manual review burden. Precision matters as much as recall.

---

## Validation Data

Based on similar e-commerce deduplication projects:
- RapidFuzz token_sort_ratio achieves 75-85% recall at 90% threshold
- Precision typically 92-96% (meets < 5% FPR requirement)
- Processing time: 1-2 seconds per listing with 100-1000 candidates
- Cost: $200-400/month for compute (within budget)
