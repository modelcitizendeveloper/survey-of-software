---
description: Budget document parsing. Camelot, Tabula, pdfplumber — financial PDF
  extraction analysis.
id: 1-302
related:
- '1.300'
- '1.301'
- '1.304'
- 1.033.1
- '1.173'
sidebar_label: 1.302 Budget Document Parsing
title: 1.302 Budget Document Parsing
weight: 1302000
---

# 1.302 Budget Document Parsing

---

<details>
<summary>S1: Rapid Discovery</summary>

# Problem: Extracting Structured Data from Government Budget Documents

## Context

Government financial documents (CAFRs, budget PDFs, financial reports) contain critical public information, but are typically published as unstructured PDFs. Extracting this data programmatically is essential for:

- **Civic transparency**: Making budget data accessible to citizens and researchers
- **Comparative analysis**: Analyzing finances across jurisdictions
- **Automated monitoring**: Tracking budget changes, debt levels, revenue trends
- **Accountability tools**: Building civic tech applications for oversight

**Current state**: No general-purpose libraries exist specifically for parsing government budget documents, despite widespread need in civic tech and public finance research.

## Problem Statement

Government budget and financial documents have unique characteristics that make them difficult to parse with general-purpose PDF tools:

### 1. Complex Multi-Page Tables
- Financial statements often span multiple pages
- Headers repeat with subtle variations
- Continuation indicators ("continued on next page")
- Page breaks mid-row or mid-section

### 2. Fund Accounting Structure
- Hierarchical account codes (e.g., 100-200-3450)
- Multiple fund types (General, Special Revenue, Capital Projects, etc.)
- Inter-fund transfers and eliminations
- Component unit reporting

### 3. Varying Formats Across Jurisdictions
- No standardized layout (unlike corporate 10-Ks)
- Different table structures for same information
- Jurisdiction-specific naming conventions
- Mix of narrative and tables

### 4. Cross-References and Footnotes
- Statement line items reference notes
- Notes contain critical context (e.g., accounting changes)
- Some figures are adjusted or restated
- Prior year comparisons

## Who Needs This?

**Civic Tech Organizations:**
- Code for America brigades building budget transparency apps
- OpenGov Foundation creating fiscal analysis tools
- Local news organizations investigating government spending

**Academic Researchers:**
- Public finance scholars comparing municipal finances
- Policy researchers analyzing budget trends
- Urban planning departments studying infrastructure investment

**Government Users:**
- Auditors comparing peer jurisdictions
- Financial officers benchmarking against similar entities
- Legislative staff analyzing budget proposals

**Commercial Applications:**
- Municipal bond analysts assessing creditworthiness
- Consulting firms evaluating government clients
- Data providers aggregating public finance data

## Why General PDF Tools Fall Short

Existing libraries (Camelot, Tabula, pdfplumber) provide low-level PDF extraction but lack:

1. **Domain knowledge**: Don't understand fund accounting structure
2. **Template handling**: Can't adapt to jurisdiction-specific formats
3. **Continuation logic**: Don't handle multi-page table assembly
4. **Semantic parsing**: Extract text but not meaning (e.g., "Fund Balance" vs "Beginning Fund Balance")
5. **Validation**: Can't verify if extraction makes financial sense

## Success Criteria

A successful solution would:
- Extract financial statements with `>95`% accuracy for common formats
- Handle at least top 100 US cities' CAFR formats
- Output structured data (JSON/DataFrame) matching fund accounting schema
- Provide confidence scores for extracted values
- Flag ambiguous or uncertain extractions for human review
- Support incremental improvement (templates, ML models)

---

# Legislative Data Access Libraries - Gap Analysis

## Quick Decision Guide

| Need | Recommendation |
|------|----------------|
| Federal bills & votes | **ProPublica Congress API** (Python: propublica-congress) |
| Comprehensive state coverage | **LegiScan API** (commercial, $$$) |
| Free state legislative data | **Open States API** (Python: pyopenstates) |
| Official Congressional data | **Congress.gov API v3** (unitedstates/congress scrapers) |
| Bill text parsing | **pdfplumber + LlamaParse** |
| International parliaments | **IPU Parline** (global) or **legislatoR** (R, 16 countries) |

## The Legislative Data Landscape (2025)

### Federal Coverage

**Excellent coverage** for U.S. Congressional data through multiple sources:
- Congress.gov API v3 (official Library of Congress)
- ProPublica Congress API (curated, enriched)
- unitedstates/congress GitHub scrapers (public domain)

### State Coverage

**Fragmented but improving**:
- Open States: Free, all 50 states + DC + PR (volunteer-powered, quality varies)
- LegiScan: Commercial, comprehensive, better timeliness ($$$ subscription)
- Direct state scrapers: Brittle, maintenance-intensive

### International Coverage

**Limited but growing**:
- IPU Parline: 269 chambers in 190 countries (official)
- Comparative Legislators Database: 16 countries, 67K+ politicians
- UK Parliament Data Platform (pdpy)
- EU legislative data (EPDB API)

## Federal: U.S. Congress APIs

### 1. Congress.gov API v3 (Official)

**Source**: Library of Congress
**Status**: Active, official government API
**Rate Limit**: 5,000 requests/hour

**Python Libraries**:
- **unitedstates/congress**: Public domain scrapers, YAML/JSON/CSV output
  - Bills, votes, amendments, legislators, committees
  - Data format: JSON + XML per bill
  - Repository: [github.com/unitedstates/congress](https://github.com/unitedstates/congress)
  - Companion: [unitedstates/congress-legislators](https://github.com/unitedstates/congress-legislators) (1789-present)

- **areed1192/united-states-congress-python-api**: Python API client
  - GovInfo.com data wrapper
  - Congressional records, hearings, treaties

**Coverage**:
- Bills: 113th Congress (2013) → present
- Legislators: 1789 → present (congress-legislators)
- Full text, status, sponsors, votes

**Strengths**:
- Official, authoritative source
- Public domain (CC0)
- Historical depth (legislators)
- Well-documented

**Limitations**:
- Coverage starts 2013 for detailed bill data
- Rate limits require API key
- Bulk downloads more efficient than API for large datasets

### 2. ProPublica Congress API

**Source**: ProPublica journalism
**Status**: Active, well-maintained
**Rate Limit**: Custom per API key

**Python Libraries**:
- **eyeseast/propublica-congress**: Most popular wrapper
  - [propublica-congress.readthedocs.io](https://propublica-congress.readthedocs.io/en/latest/)
  - PyPI: `pip install propublica-congress`
  - Organized subclients per endpoint
  - httplib2 with pluggable caching (FileCache default, Redis recommended)

- **MikeBoris/propublica**: Minimal wrapper
- **tomigee/Congress**: Alternative wrapper

**R Library**:
- **ProPublicaR**: CRAN package for R users
  - Bill searches by keyword
  - Member sponsorship/vote comparison
  - Updated July 2025

**Coverage**:
- Members, votes, bills, nominations
- Bill subjects, amendments, related bills
- Personal explanations
- Committee assignments

**Strengths**:
- Enriched data (curated by journalists)
- Active maintenance
- Good documentation
- Multi-language support (Python, R)

**Limitations**:
- Depends on ProPublica's curation pipeline
- Requires API key application
- Less historical depth than Congress.gov

## State: Legislative Data APIs

### 3. Open States API v3

**Source**: Plural Policy (nonprofit, acquired 2021)
**Status**: Active, volunteer-powered
**Coverage**: All 50 states + DC + Puerto Rico

**Python Library**:
- **pyopenstates**: Official Python client
  - [openstates.github.io/pyopenstates](https://openstates.github.io/pyopenstates/)
  - PyPI: `pip install pyopenstates`
  - Environment: `OPENSTATES_API_KEY`
  - API root: https://v3.openstates.org/

**Data Types**:
- Bill summaries, full text, status
- Votes (roll calls)
- Sponsorships
- Legislator profiles
- Committee assignments
- Events, hearings

**Strengths**:
- Free and open (nonprofit mission)
- Standardized format across states
- RESTful JSON API
- Bulk downloads available
- Active community

**Limitations**:
- **Data quality varies by state** (volunteer scrapers)
- **Timeliness issues**: Some states lag 48+ hours
- **Historical gaps**: Not all states preserve old legislator/committee data
- **Scraper brittleness**: State website changes break scrapers
- **Missing roll calls**: Some states don't publish vote details
- **Maintenance burden**: Core developers have day jobs

**Quality Report Card Issues**:
- Some states have incomplete bill data
- JavaScript-heavy sites difficult to scrape
- Flash/legacy tech causes problems
- Bookmarking bills sometimes impossible
- Historical archives incomplete

Source: [Open States Legislative Data Report Card](https://open.pluralpolicy.com/reportcard/)

### 4. LegiScan API (Commercial)

**Source**: LegiScan, Inc.
**Status**: Active, commercial service
**Coverage**: All 50 states + Congress

**Pricing Tiers**:
- **Public (Free)**: 30,000 queries/month, pull interface
- **Pull Subscription**: 100K-250K queries/month
- **Push/Enterprise**: Full database replication, 4-hour updates (15-min optional)

**Data Formats**: JSON, XML, CSV

**API Flavors**:
- **Pull Interface**: Query-based API
- **Push Interface**: Webhook/replication

**Coverage**:
- Bills: Detail, status, history
- Full text: PDF, HTML, XML
- Roll calls: Vote records
- Sponsors: All types
- Current & historical archives

**Strengths**:
- **Comprehensive**: Best state coverage
- **Timely**: 4-hour update SLA (enterprise: 15-min)
- **Uniform format**: Consistent across states
- **Structured JSON**: Easy to parse
- **Full text search**: Built-in search capability
- **Commercial support**: Paid service = reliability

**Limitations**:
- **Cost**: Significant for high-volume use
- **Proprietary**: Not open source
- **Vendor lock-in**: Commercial dependency

**Use Cases**:
- Commercial products (lobbying, compliance, civic tech)
- Analysis projects requiring high quality
- Organizations needing SLA guarantees

Sources:
- [LegiScan API](https://legiscan.com/legiscan)
- [LegiScan API User Manual (PDF)](https://api.legiscan.com/dl/LegiScan_API_User_Manual.pdf)

## Bill Text Extraction & Parsing

### Challenge: Legislative PDFs Are Hard

Standard diff tools don't work for legislation because:
- Matching "same section" requires semantic judgment
- Amendatory language is legal prose, not code
- Citations must be machine-processable
- Multiple change types (amendments, hearing notes, votes, testimony)

Sources: [Version Control for Law - Data Foundation](https://datafoundation.org/news/blogs/335/335-Version-Control-for-Law-Tracking-Changes-in-the-US-Congress)

### Python PDF Parsing Libraries

**Basic Text Extraction**:
- **PyPDF**: Oldest, lightweight, limited layout handling
- **PyMuPDF (Fitz)**: Powerful, multi-format (PDF, EPUB, etc.)
- **PDFMiner**: Robust for complex layouts, metadata extraction
- **pdfplumber**: Best for tabular data + text
  - [github.com/jsvine/pdfplumber](https://github.com/jsvine/pdfplumber)
  - Extracts char-level detail, rectangles, lines
  - Excellent table extraction

**Legislative-Specific**:
- **LlamaParse + Python**: Best parsing performance for legislative PDFs
  - Extracts to structured Markdown
  - Inserts footnotes at reference points
  - MarkdownNodeParser for splitting
  - Structured JSON output
  - Source: [Parsing Legislative Consultation PDFs - Medium](https://medium.com/@thiloweber/parsing-legislative-consultation-pdfs-using-llamaparse-fdd4627b9094)

- **camelot-py**: Table extraction specialist
  - Best with PDFs having clear table lines

- **LangExtract** (Google): LLM-based structured extraction
  - [github.com/google/langextract](https://github.com/google/langextract)
  - User-defined instructions
  - Source grounding (links extracted data to original text)
  - Good for clinical notes, reports (adaptable to legislative docs)

### Version Control & Diff Tools

**Problem**: Legislative diffs need semantic understanding
- Amendments reference sections by citation
- Machine-readable citations + amendatory phrases → executable instructions
- Goal: Legally-relevant diff, not text diff

**Solutions**:
- **unitedstates/congress issue #210**: Generate view-friendly diffs from legislation
  - Community effort to solve diff problem
  - [github.com/unitedstates/congress/issues/210](https://github.com/unitedstates/congress/issues/210)

- **Commercial tools** (StateScape, Propylon): Side-by-side marked-up versions
  - Show current vs. amended text
  - Manual review required

**Gap**: No robust open-source legislative diff tool

## International: Comparative Legislative Data

### 5. IPU Parline Database

**Source**: Inter-Parliamentary Union (global organization)
**Status**: Active, redeveloped April 2024
**Coverage**: 269 chambers in 190 countries

**Features**:
- 650 data points per chamber
- Data explorer + API + data dictionary
- Comparative data on all national parliaments since 1996

**Strengths**:
- Official global authority
- Comprehensive coverage
- API access

**Limitations**:
- Focus on chamber-level data, not individual bills
- Less granular than country-specific APIs

Source: [IPU Parline](https://data.ipu.org/)

### 6. Comparative Legislators Database (CLD)

**R Library**: **legislatoR**
- CRAN + GitHub: [github.com/saschagobel/legislatoR](https://github.com/saschagobel/legislatoR)
- Coverage: 16 countries, 67,000+ politicians
- Data: Political, sociodemographic, career, online presence, public attention, visual

**Strengths**:
- Rich individual legislator data
- Historical + contemporary
- Multi-country standardized format

**Limitations**:
- **R only** (no Python equivalent)
- 16 countries (limited compared to IPU)

Source: [The Comparative Legislators Database - Cambridge](https://www.cambridge.org/core/journals/british-journal-of-political-science/article/abs/comparative-legislators-database/D28BB58A8B2C08C8593DB741F42C18B2)

### 7. Country-Specific APIs

**UK Parliament**:
- **pdpy**: Python package for UK Parliament Data Platform
  - [github.com/houseofcommonslibrary/pdpy](https://github.com/houseofcommonslibrary/pdpy)
  - SPARQL interface + high-level functions
  - Returns pandas DataFrames

**European Union**:
- **EPDB API**: EU legislative data as JSON
  - Machine-readable metadata
  - [api.epdb.eu](http://api.epdb.eu/)

## Gap Analysis: What's Missing?

### 1. State Legislative Data Quality (Biggest Gap)

**Problem**:
- Open States relies on fragile web scrapers
- State websites change → scrapers break
- Quality varies dramatically by state
- Roll call votes often missing
- Historical data incomplete (old legislators, committees)
- Timeliness: Some states lag 48+ hours

**What's needed**:
- **State-mandated data APIs**: Require legislatures to publish structured data
- **Standard format**: OpenLegislation.org proposed (not widely adopted)
- **Funding**: Open States needs resources for faster scraper fixes
- **Better scraper frameworks**: More resilient to website changes

### 2. Legislative Diff & Version Control (Critical Gap)

**Problem**:
- No robust open-source bill diff tool
- Standard text diff doesn't understand legislative semantics
- Tracking amendments across versions is manual

**What's needed**:
- **Legislative-aware diff engine**: Understands citations, sections, amendments
- **Version control for law**: Integrate with Git-like workflows
- **Machine-executable amendments**: Parse amendatory language into structured edits
- **Visualization**: Show how bills evolve over time

### 3. Historical State Legislative Data (Major Gap)

**Problem**:
- Most states don't preserve historical committee/legislator data
- Bill archives incomplete
- No centralized historical database for state legislatures

**What's needed**:
- **Digitization projects**: Scan and OCR historical legislative records
- **Standardized archival format**: Ensure future data is preserved
- **Academic partnerships**: Universities host/curate historical data

### 4. Cross-State Bill Tracking (Model Legislation Gap)

**Problem**:
- Model legislation (ALEC, etc.) spreads across states
- No easy way to track "same bill, different state"
- Text reuse detection requires custom pipelines

**What exists**:
- LegiScan full-text search (commercial)
- Custom Solr-based text reuse pipelines (investigative journalism)
  - Source: [Checking for legislative text reuse - investigate.ai](https://investigate.ai/azcentral-text-reuse-model-legislation/05-checking-for-legislative-text-reuse-using-python-solr-and-simple-text-search/)

**What's needed**:
- **Open text reuse API**: Track model legislation spread
- **Bill fingerprinting**: Semantic similarity, not just text match
- **Network analysis tools**: Visualize bill spread across states

### 5. Real-Time Legislative Alerts (Integration Gap)

**Problem**:
- Most APIs are pull-based (query for updates)
- Real-time alerts require polling or commercial services (LegiScan push)

**What's needed**:
- **Webhook/push APIs**: Open States, Congress.gov push updates
- **Event streaming**: Kafka/RSS for legislative events
- **Standardized alert format**: Common schema for notifications

### 6. Local Government Data (Massive Gap)

**Problem**:
- Federal: Excellent
- State: Fragmented but available
- **County/City/Municipality: Almost nothing**

**What's missing**:
- County board minutes, resolutions, ordinances
- City council legislation
- Local ballot measures
- Municipal budget data

**Exception**:
- Some civic tech projects (Digital Democracy for CA)
- One-off scrapers for major cities

**What's needed**:
- **Local legislative data standard**: Extend Open States model
- **Municipal API coalition**: Cities publish structured data
- **Civic tech funding**: Support local scraper development

### 7. International Coverage (Python Gap)

**Problem**:
- **legislatoR** (Comparative Legislators DB) is R-only
- No comprehensive Python library for multi-country legislative data

**What's needed**:
- **pylegislatoR**: Python port of legislatoR
- **Global legislative API aggregator**: Unified interface to country-specific APIs
- **International bill corpus**: Standardized dataset for research

### 8. Lobbying & Campaign Finance Integration (Data Silo Gap)

**Problem**:
- Legislative data separate from lobbying/finance data
- No integrated APIs for "who sponsored bill X + who donated to them"

**What exists**:
- OpenSecrets.org (campaign finance)
- Senate lobbying disclosure
- Separate databases, manual joins

**What's needed**:
- **Integrated legislative influence API**: Bills + votes + donors + lobbyists
- **Graph database**: Relationship mapping (legislator-donor-bill-committee)

## Recommendations by Use Case

### For Academic Research

**Federal**: unitedstates/congress scrapers + bulk downloads
- Public domain, historical depth, reproducible

**State**: Open States bulk downloads + LegiScan for gaps
- Open States for general analysis
- LegiScan subscription if timeliness/quality critical

**International**: legislatoR (R) or IPU Parline (API)

### For Civic Engagement Tools

**Federal**: ProPublica Congress API
- Enriched data, good documentation

**State**: Open States API
- Free, all states, JSON format

**Local**: Custom scrapers (no standardized source)

### For Commercial Compliance/Lobbying

**Federal + State**: LegiScan API
- Comprehensive, timely, SLA guarantees
- Worth the cost for business-critical use

### For Investigative Journalism

**Federal**: unitedstates/congress + ProPublica
- Combine official + curated sources

**State**: LegiScan (bulk download) + custom text reuse pipeline
- Full-text search for model legislation
- Solr/Elasticsearch for similarity detection

**Tools**: pdfplumber + LlamaParse for PDF extraction

### For Budget/Finance Analysis

**Federal**: Congress.gov API (appropriations bills)
**State**: LegiScan + Open States (budget bills)
**Gap**: Need to parse budget tables from PDFs
- Use pdfplumber + camelot for table extraction

**Local**: Almost no standardized data (manual collection)

## Technology Stack Recommendations

### Python Stack (Federal Focus)
```python
# Federal legislation
from propublica import Congress
from congress_api import CongressAPI  # unitedstates wrapper

# State legislation
import pyopenstates

# PDF parsing
import pdfplumber
from llama_parse import LlamaParse  # For complex legislative PDFs

# Text analysis
import spacy  # NER for legislator/org extraction
from sentence_transformers import SentenceTransformer  # Bill similarity
```

### R Stack (Comparative/Academic)
```r
library(ProPublicaR)    # Federal data
library(legislatoR)     # International comparative data
library(congress)       # Congress.gov API (CRAN)
```

### Commercial Stack (Production)
- LegiScan API (Python/R/REST)
- StateScape (legislative tracking platform)
- Quorum (government affairs software)

## Key Insights

### 1. Federal > State > Local (Coverage Hierarchy)
- Federal: Excellent APIs, multiple sources, well-documented
- State: Fragmented, quality varies, free options exist but brittle
- Local: Almost nonexistent, manual collection required

### 2. Open Source vs. Commercial Trade-off
- **Open States**: Free but quality/timeliness varies
- **LegiScan**: Expensive but comprehensive and reliable
- **Hybrid approach**: Open States for exploratory work, LegiScan for production

### 3. PDF Parsing is Still a Challenge
- Legislative PDFs are complex (tables, footnotes, amendments)
- LlamaParse + pdfplumber combo works best
- No perfect solution for all PDF varieties

### 4. Version Control for Law is Unsolved
- Biggest open research problem
- Legal semantics ≠ text diff
- Community working on it (unitedstates/congress issue #210)

### 5. Model Legislation Tracking Requires Custom Pipelines
- No turnkey solution
- Investigative journalists build custom Solr/Elasticsearch pipelines
- Opportunity for open-source tooling

### 6. International Coverage Lacks Python Support
- legislatoR (R) is state-of-the-art for comparative data
- Python users stuck with country-specific APIs
- IPU Parline fills chamber-level gap but not bill-level

### 7. Real-Time Alerts Require Polling (Mostly)
- Most APIs are pull-based
- LegiScan offers push (commercial)
- Webhook/streaming gap for open-source options

## Data Quality Issues (Open States)

From community reports and official documentation:

### Technical Challenges
- **Scraper brittleness**: State website redesigns break scrapers
- **JavaScript-heavy sites**: Difficult to scrape without headless browsers
- **Legacy tech (Flash)**: Some states still use outdated platforms
- **Bookmarking issues**: Some state sites don't allow direct bill links

### Data Completeness
- **Missing roll calls**: Not all states publish detailed vote records
- **Historical gaps**: Old legislators/committees not archived
- **Timeliness**: 48+ hour delays for some states
- **Prefiled bills**: Occasionally missed during ingestion

### Maintenance Burden
- All core Open States developers have day jobs (volunteer project)
- Scraper fixes can lag behind state website changes
- Quality audits help but resource-constrained

Source: [Open States GitHub Issues](https://github.com/openstates/issues)

## Future Directions

### Standards & Advocacy
- Push for state-mandated data APIs (like congress.gov)
- Adopt common legislative data standards
- Fund Open States to improve infrastructure

### Tool Development
- Legislative diff engine (semantic understanding)
- Bill similarity/text reuse API
- Integrated lobbying+finance+legislation graph database

### Coverage Expansion
- Local government data coalition
- Python port of legislatoR
- Historical state legislative archives

### Real-Time Infrastructure
- Webhook-based legislative event streaming
- RSS/Atom feeds for bill updates
- Kafka/event-driven architecture for alerts

## Sources

### APIs & Documentation
- [Congress.gov API - Library of Congress](https://www.loc.gov/apis/additional-apis/congress-dot-gov-api/)
- [ProPublica Congress API](https://projects.propublica.org/api-docs/congress-api/)
- [Open States API v3](https://docs.openstates.org/api-v3/)
- [LegiScan API](https://legiscan.com/legiscan)
- [IPU Parline Database](https://data.ipu.org/)

### Python Libraries
- [propublica-congress - Read the Docs](https://propublica-congress.readthedocs.io/en/latest/)
- [pyopenstates Documentation](https://openstates.github.io/pyopenstates/)
- [unitedstates/congress - GitHub](https://github.com/unitedstates/congress)
- [unitedstates/congress-legislators - GitHub](https://github.com/unitedstates/congress-legislators)
- [pdfplumber - GitHub](https://github.com/jsvine/pdfplumber)
- [LangExtract - GitHub](https://github.com/google/langextract)

### R Libraries
- [ProPublicaR - CRAN](https://cran.r-project.org/web/packages/ProPublicaR/ProPublicaR.pdf)
- [legislatoR - GitHub](https://github.com/saschagobel/legislatoR)
- [congress - CRAN](https://cran.r-project.org/web/packages/congress/congress.pdf)

### International APIs
- [UK Parliament pdpy - GitHub](https://github.com/houseofcommonslibrary/pdpy)
- [EU Legislative Data API](http://api.epdb.eu/)

### Analysis & Reports
- [Open States Legislative Data Report Card](https://open.pluralpolicy.com/reportcard/)
- [How Accessible Are Online Legislative Data Archives? - Oxford Internet Institute](https://blogs.oii.ox.ac.uk/policy/how-accessible-are-online-legislative-data-archives-to-political-scientists/index.htm)
- [Version Control for Law - Data Foundation](https://datafoundation.org/news/blogs/335/335-Version-Control-for-Law-Tracking-Changes-in-the-US-Congress)
- [Checking for Legislative Text Reuse - investigate.ai](https://investigate.ai/azcentral-text-reuse-model-legislation/05-checking-for-legislative-text-reuse-using-python-solr-and-simple-text-search/)
- [Parsing Legislative PDFs with LlamaParse - Medium](https://medium.com/@thiloweber/parsing-legislative-consultation-pdfs-using-llamaparse-fdd4627b9094)
- [Best Python PDF to Text Parser Libraries - Unstract](https://unstract.com/blog/evaluating-python-pdf-to-text-libraries/)

### Project Pages
- [@unitedstates Project](https://unitedstates.github.io/)
- [Open States Project](https://docs.openstates.org/)
- [LegiScan Datasets](https://legiscan.com/datasets)
- [Plural Policy (Open States)](https://pluralpolicy.com/open/)

---

# USAspending.gov API Tooling: Gap Analysis

## Executive Summary

USAspending.gov provides comprehensive U.S. federal spending data through a public RESTful API, but the ecosystem of tools for accessing and analyzing this data is **severely underdeveloped**. While the API itself is robust and well-documented, practitioners face significant barriers:

- **No mature Python/R libraries**: Existing wrappers are abandoned or minimal
- **No entity resolution layer**: Duplicate/inconsistent vendor records are common
- **No caching infrastructure**: Repeated queries hammer the API unnecessarily
- **No historical tracking**: No tools for longitudinal analysis or change detection
- **High barrier to entry**: Direct API usage requires significant domain knowledge

This represents a major opportunity for building a comprehensive wrapper that would dramatically improve researcher and analyst productivity.

---

## 1. USAspending.gov API Overview

### API Basics

- **URL**: https://api.usaspending.gov/
- **Version**: V2 (V1 deprecated)
- **Authentication**: None required (public API)
- **Rate Limits**: Not prominently documented; general api.data.gov rate limits may apply
- **Data Coverage**: FY2001-present (custom awards), FY2008-present (bulk downloads)
- **Open Source**: Full source code at [fedspendingtransparency/usaspending-api](https://github.com/fedspendingtransparency/usaspending-api)

### Data Sources

The API aggregates data from multiple authoritative sources:

- **FPDS (Federal Procurement Data System)**: Contract data (180+ data points)
- **FABS (Financial Assistance Broker Submission)**: Grant, loan, and financial assistance data
- **SAM.gov**: Entity registration and unique identifiers (UEI)
- **GSDM (Governmentwide Spending Data Model)**: Standardized data definitions

### What's Available

- **Awards**: Contracts, grants, loans, financial assistance
- **Entities**: Recipient organizations, agencies, subrecipients
- **Geography**: State, congressional district, ZIP code breakdowns
- **Accounts**: Budget function, object class, federal account data
- **Transactions**: Individual modifications and subawards
- **Disaster Spending**: COVID-19, natural disasters (separate endpoints)

### Official Documentation

- [USAspending API Documentation](https://api.usaspending.gov/)
- [Analyst's Guide to Federal Spending Data](https://www.usaspending.gov/federal-spending-guide)
- [Basic USAspending API Training (PDF)](https://www.usaspending.gov/data/Basic-API-Training.pdf)
- [Data Dictionary](https://www.usaspending.gov/data-dictionary)
- [GSDM (Data Model Specification)](https://tfx.treasury.gov/data-transparency/gsdm)

---

## 2. Existing Python Libraries

### codeforamerica/usa_spending_python

**Repository**: https://github.com/codeforamerica/usa_spending_python

**Status**: **Effectively abandoned**
- Last commit: February 21, 2021 (4 years ago)
- 15 total commits
- 6 stars, 2 forks
- Zero open issues/PRs

**Functionality**:
- Single `Contracts` class for searching contracts
- Filter by: state, ZIP code, year, date range, competition type
- Basic pagination support
- Example: `Contracts(zipcode=12345, year=2010)`

**Limitations**:
- Pre-dates USAspending V2 API (likely targets V1)
- No support for grants, loans, or financial assistance
- No entity resolution
- No caching
- No bulk download support
- No historical tracking
- Minimal error handling

**Verdict**: Not suitable for production use. Would require complete rewrite.

### Other Python Resources

**bsweger/usaspending-scripts**: https://github.com/bsweger/usaspending-scripts
- Scripts (not a library) for downloading and summarizing data
- Ad-hoc utilities, not a reusable API wrapper

**Data4Democracy/usaspending**: https://github.com/Data4Democracy/usaspending
- Exploratory project for collaborative analysis
- No formal library structure

**unitedstates/federal_spending**: https://github.com/unitedstates/federal_spending
- Data importer focused on downloading/normalizing raw data
- Not an API wrapper

---

## 3. Existing R Packages

### No Formal R Package Exists

Unlike Python, R has **no dedicated USAspending package on CRAN**.

**Workarounds**:
- Direct API calls using `httr`, `jsonlite`
- [RPubs tutorial: Querying USAspending API from RStudio](https://rpubs.com/cenuno/USAspendingAPI)
- [Official R markdown examples](https://github.com/fedspendingtransparency/analytics-usaspending-website) in USAspending's analytics repo

**Implication**: R users face even higher barriers than Python users.

---

## 4. Current State of Tooling: Summary

| Capability | Python | R | Status |
|------------|--------|---|---------|
| **API Wrapper** | Abandoned (2021) | None | ❌ Critical gap |
| **Entity Resolution** | None | None | ❌ Critical gap |
| **Caching Layer** | None | None | ❌ Critical gap |
| **Historical Tracking** | None | None | ❌ Critical gap |
| **Bulk Downloads** | Manual only | Manual only | ⚠️ Partial gap |
| **Data Validation** | None | None | ❌ Critical gap |
| **Documentation** | Ad-hoc notebooks | Ad-hoc scripts | ⚠️ Partial gap |

**Key Finding**: The ecosystem relies on **ad-hoc scripts and Jupyter notebooks** rather than reusable libraries. Every analyst rebuilds the same infrastructure from scratch.

---

## 5. Gap Analysis: What Should Exist But Doesn't

### 5.1 Pythonic API Wrapper

**What's Missing**:
- Modern Python library (3.9+) with type hints
- Pydantic models for all API responses
- Intuitive query builder (not raw JSON payloads)
- Automatic pagination handling
- Retry logic with exponential backoff
- Async support for bulk queries
- Comprehensive error handling

**Example of what SHOULD exist**:
```python
from usaspending import USAspending

client = USAspending()

# Intuitive query interface
awards = client.awards.search(
    agency="Department of Defense",
    award_type="contracts",
    date_range=("2023-01-01", "2023-12-31"),
    location="CA"
)

# Automatic pagination
for award in awards:  # Handles pagination transparently
    print(award.recipient, award.amount)
```

**Current Reality**: Users must manually construct JSON payloads, handle pagination, and parse responses.

---

### 5.2 Entity Resolution Layer

**Problem**: Federal spending data has severe entity quality issues:

- **Pre-2022**: Used DUNS numbers (proprietary Dun & Bradstreet IDs)
- **Post-2022**: Migrated to UEI (Unique Entity Identifier) from SAM.gov
- **Transition chaos**: Many records lack UEI, some have both, inconsistent backfilling
- **Name variations**: "IBM", "IBM Corporation", "International Business Machines", "I.B.M. Corp" all appear as distinct entities
- **Missing data**: GAO reports 19 of 101 agencies didn't submit required linking files (as of 2021)
- **Duplicates**: Grant subawards have "likely duplicative records" (GAO-24-106214)

**What's Missing**:
- Entity disambiguation service
- Fuzzy name matching
- Parent/subsidiary relationship tracking
- Historical entity ID mapping (DUNS → UEI)
- Data quality scoring per entity
- Vendor deduplication

**Impact**: Without entity resolution, queries like "how much does Lockheed Martin receive?" produce inaccurate results due to name variations and subsidiary structures.

**References**:
- [GAO: Data Quality Issues Impact Usefulness](https://www.gao.gov/blog/usaspending.gov-offers-insight-government-spending-data-quality-and-user-awareness-impact-its-usefulness)
- [GAO-24-106214: Opportunities to Improve USAspending.gov Data](https://www.gao.gov/products/gao-24-106214)
- [GSA: Unique Entity ID Transition](https://www.gsa.gov/about-us/organization/federal-acquisition-service/fas-initiatives/integrated-award-environment/iae-systems-information-kit/unique-entity-id-is-here)

---

### 5.3 Intelligent Caching Infrastructure

**Problem**: USAspending data changes slowly (monthly updates from FPDS), but queries are expensive:

- Large result sets (millions of records)
- Complex filters across multiple dimensions
- No built-in caching at API level

**What's Missing**:
- Local SQLite cache with automatic expiration
- Query result caching with cache keys
- Incremental updates (only fetch new records since last query)
- Cache invalidation policies
- Offline mode for cached data

**Example**:
```python
# First query: hits API, caches results
results = client.awards.search(..., cache=True, cache_ttl="30d")

# Subsequent identical queries: instant from cache
results = client.awards.search(...)  # <1ms instead of 10s

# Incremental updates
results = client.awards.search(..., since_last_update=True)  # Only new records
```

**Impact**: Analysts waste time re-running identical queries. Development iterations are slow. API gets hammered unnecessarily.

---

### 5.4 Historical Tracking & Longitudinal Analysis

**Problem**: USAspending provides **point-in-time snapshots**, not **change tracking**:

- Award amounts get modified over time (via modifications/amendments)
- No easy way to track: "What changed between Q1 2023 and Q1 2024?"
- No change detection notifications
- Subaward data can be corrected retroactively

**What's Missing**:
- Time-series database for tracking changes
- Diff detection (what changed since last snapshot?)
- Alert system for monitoring specific contracts/vendors
- Trend analysis tools (spending velocity, award size distributions over time)
- Historical amendment tracking

**Use Cases**:
- "Alert me when DoD awards to Vendor X exceed $10M/month"
- "Show me how this contract's total value changed over 5 years"
- "Which agencies increased/decreased spending on cybersecurity from 2022-2024?"

**Current Workaround**: Manual snapshots and spreadsheet comparisons.

---

### 5.5 Data Validation & Quality Scoring

**Problem**: USAspending data quality varies significantly by agency and field:

- **Completeness issues**: Missing required fields, null subaward amounts
- **Accuracy issues**: "Impossibly large subaward amounts" (GAO finding)
- **Timeliness issues**: Agencies have 30 days to report but many are late
- **Consistency issues**: Same contract reported differently across systems

**What's Missing**:
- Data quality metrics per record/field
- Validation rules based on GSDM specifications
- Quality scores (0-100) for each award
- Known issue database (which agencies/fields are unreliable?)
- Data cleaning utilities

**Example**:
```python
award = client.awards.get("CONT_AWD_12345")
print(award.quality_score)  # 87/100
print(award.quality_issues)
# [
#   "Missing subaward data",
#   "Recipient address incomplete",
#   "Reporting agency 14 days late"
# ]
```

**References**:
- [GAO: Data Quality Challenges Remain](https://www.gao.gov/products/gao-22-104702)
- [CRS Report: Data Source Limitations](https://www.congress.gov/crs-product/R44027)

---

### 5.6 Bulk Download Manager

**Problem**: USAspending offers bulk downloads (agency, fiscal year), but:

- Downloads are multi-GB ZIP files
- No programmatic access to bulk downloads
- No incremental updates (must re-download entire file)
- No schema validation tools

**What's Missing**:
- Bulk download CLI tool
- Automatic extraction and loading into local database
- Incremental sync (only download deltas)
- Schema evolution handling (field name changes between years)

---

### 5.7 Domain-Specific Query Builders

**Problem**: Common analytical tasks require complex multi-step queries:

- "Find all DoD contracts to small businesses in cybersecurity"
- "Compare grant spending across states for education programs"
- "Track disaster relief spending for Hurricane Ian"

**What's Missing**:
- Pre-built query templates for common analyses
- Domain-specific filters (e.g., "small_business=True" abstracts NAICS codes)
- Award type-specific interfaces (contracts vs. grants have different fields)
- Geographic analysis helpers (congressional district lookups, state rollups)

---

## 6. Alternative Approaches

### 6.1 Direct API Usage

**Pros**:
- No dependencies
- Full control over queries
- Immediate access to new API features

**Cons**:
- High development overhead (every analyst rebuilds the same code)
- No caching or optimization
- Requires deep domain knowledge (GSDM, FPDS, FABS schemas)
- Error-prone (manual JSON construction)

**Best For**: One-off exploratory queries

---

### 6.2 Commercial Tools

Several commercial platforms provide curated federal spending data:

#### GovTribe
- **URL**: https://about.govexec.com/govtribe/
- **Features**: Machine learning-driven opportunity recommendations, vendor profiles, pipeline management
- **Voted**: "Favorite Federal Market Intelligence Tool" by GovBrew readers (80% of vote)
- **Target Users**: Government contractors doing business development

#### FedScout
- **URL**: https://www.fedscout.com/about
- **Features**: Contract search, win probability estimation, education resources for new vendors
- **Focus**: Helping new entrants understand federal market (SBIR, OTAs, grants, contracts)

#### Federal Compass
- **URL**: https://www.federalcompass.com
- **Features**: Market intelligence, contractor tracking, spending analytics

#### GovSpend
- **URL**: https://govspend.com/
- **Features**: B2G intelligence for public sector

**Pros**:
- Pre-cleaned data
- Entity resolution included
- User-friendly interfaces
- Historical tracking built-in

**Cons**:
- **Expensive**: $1,000s-$10,000s/year per user
- **Proprietary**: No API access, can't export clean data
- **Black box**: Unknown data processing methods
- **Vendor focus**: Optimized for contractors, not researchers/analysts
- **Limited customization**: Can't add custom metrics or analyses

**Best For**: Government contractors with budget, not academic researchers or civic tech projects.

**References**:
- [GovTribe Press Release](https://about.govexec.com/company/blog/govtribe-voted-favorite-federal-market-intelligence-tool-by-govbrew-readers-copy-2/)
- [FedScout About Page](https://www.fedscout.com/about)

---

### 6.3 Manual Bulk Downloads

**Process**:
1. Download multi-GB ZIP files from https://www.usaspending.gov/download_center
2. Extract CSV files
3. Load into Excel/Pandas/R
4. Manually join across files
5. Repeat monthly for updates

**Pros**:
- Offline analysis
- No rate limits

**Cons**:
- Huge storage requirements (10s-100s of GB)
- Manual update process
- No incremental updates
- Schema changes break pipelines

**Best For**: One-time large-scale analysis with stable scope

---

### 6.4 FPDS/FABS Direct Access

**Option**: Query authoritative sources directly instead of USAspending:
- **FPDS-NG**: https://www.fpds.gov/ (contracts)
- **FABS**: Data submitted via DATA Act Broker

**Pros**:
- Most authoritative source
- More granular data

**Cons**:
- Even more complex schemas
- Requires separate queries to multiple systems
- No unified interface
- USAspending already aggregates these

**Best For**: Agency-specific compliance work, not general analysis

---

## 7. Complexity of Building a Comprehensive Wrapper

### 7.1 Technical Complexity

| Component | Complexity | Effort Estimate |
|-----------|------------|-----------------|
| Basic API wrapper | Low | 2-4 weeks |
| Pagination handling | Low | 1 week |
| Type hints + Pydantic models | Medium | 3-4 weeks |
| Caching layer (SQLite) | Medium | 3-4 weeks |
| Entity resolution (fuzzy matching) | High | 6-8 weeks |
| Historical tracking (time-series DB) | High | 6-8 weeks |
| Bulk download manager | Medium | 3-4 weeks |
| Data validation framework | Medium | 4-6 weeks |
| Documentation + examples | Medium | 3-4 weeks |
| Test suite (unit + integration) | Medium | 4-6 weeks |

**Total Estimate**: 4-6 months for single developer to reach "production-ready" status.

### 7.2 Domain Knowledge Requirements

**Essential Knowledge**:
- USAspending API V2 endpoints (20+ endpoint families)
- GSDM data model (100s of fields)
- FPDS vs. FABS schemas (different for contracts vs. grants)
- SAM.gov entity structure
- Award lifecycle (base award → modifications → amendments)
- Congressional district mappings (historical changes)
- NAICS codes (industry classification)
- PSC codes (product/service codes for contracts)
- CFDA numbers (Catalog of Federal Domestic Assistance for grants)

**Challenges**:
- Documentation is scattered across USAspending, Treasury, GSA sites
- Schema changes between fiscal years
- Inconsistent field naming (snake_case vs. camelCase)
- Implicit relationships not documented (e.g., parent awards)

### 7.3 Data Quality Challenges

**Known Issues to Handle**:
- Missing DUNS/UEI in older records
- Duplicate subawards
- Impossibly large amounts (data entry errors)
- Incomplete recipient addresses
- Late reporting (30-day window often exceeded)
- Agency systems not fully integrated
- Congressional district changes (2023 redistricting)

**Implication**: A naive wrapper will propagate garbage data. A good wrapper must detect and flag quality issues.

### 7.4 Maintenance Burden

**Ongoing Work Required**:
- Track API changes (USAspending updates monthly)
- Handle schema evolution (new fields, deprecated fields)
- Update NAICS/PSC code mappings (change annually)
- Maintain entity resolution rules
- Monitor data quality regressions
- Update documentation

**Estimate**: 1-2 days/month ongoing maintenance.

---

## 8. Recommended Path Forward

### Phase 1: Minimal Viable Product (8-10 weeks)
1. **Core API wrapper** with automatic pagination
2. **Type-safe models** using Pydantic
3. **Basic caching** (in-memory + disk)
4. **Awards & Recipients** endpoints (most common use cases)
5. **Comprehensive test suite**
6. **Example notebooks** for common tasks

**Deliverable**: `pip install usaspending-api` that replaces 90% of ad-hoc scripts.

### Phase 2: Entity Resolution (6-8 weeks)
1. **Fuzzy name matching** using RapidFuzz
2. **DUNS → UEI mapping** table
3. **Vendor deduplication** heuristics
4. **Parent/subsidiary tracking**
5. **Quality scoring** per entity

**Deliverable**: Accurate "total spending to vendor X" queries.

### Phase 3: Historical Tracking (6-8 weeks)
1. **Time-series database** (e.g., TimescaleDB extension for PostgreSQL)
2. **Snapshot system** for awards
3. **Change detection** (diff between snapshots)
4. **Alert framework** for monitoring
5. **Trend analysis** utilities

**Deliverable**: Longitudinal analysis toolkit.

### Phase 4: Advanced Features (8-12 weeks)
1. **Bulk download manager**
2. **R package** (reticulate wrapper or native implementation)
3. **Domain-specific query builders**
4. **Data validation framework**
5. **Web dashboard** for interactive exploration

**Deliverable**: Enterprise-grade research platform.

---

## 9. Competitive Landscape

### Open Source
- **None** comparable for USAspending
- Potential inspiration:
  - `pygithub` (mature API wrapper pattern)
  - `openai` (good async implementation)
  - `anthropic` (excellent type hints + error handling)

### Commercial
- **Dominant players**: GovTribe, FedScout, Federal Compass
- **Market size**: Unknown but likely $10M-$50M/year (based on pricing × estimated users)
- **Moat**: Cleaned data + entity resolution + user-friendly UIs
- **Vulnerability**: High pricing, vendor focus, no API access

**Opportunity**: An open-source library with entity resolution could:
- Democratize federal spending analysis
- Enable civic tech projects
- Support academic research
- Undercut commercial tools for basic use cases

---

## 10. Key Takeaways

### Findings

1. **USAspending API is robust** but tooling ecosystem is immature
2. **No production-ready libraries** exist in Python or R
3. **Entity resolution is the killer feature** commercial tools have, open source lacks
4. **Data quality issues** require more than a naive API wrapper
5. **High value, medium complexity** project (4-6 months to MVP+)

### Recommendations

**For Researchers**:
- Use direct API access for one-off queries
- Consider commercial tools if budget allows and entity resolution is critical
- Advocate for open-source library development

**For Developers**:
- Focus on entity resolution (highest value, hardest to build)
- Start with contracts/awards (most common use case)
- Prioritize type safety and caching (avoid common pitfalls)
- Build in quality scoring from day 1 (don't propagate bad data)

**For Funders**:
- **High ROI opportunity**: A comprehensive library would benefit thousands of analysts
- Potential users: journalists, academics, civic tech, small contractors, government accountability orgs
- Estimated impact: $1M-$5M/year in saved analyst time (conservative)

---

## Sources

### Official Documentation
- [USAspending API](https://api.usaspending.gov/)
- [Analyst's Guide to Federal Spending Data](https://www.usaspending.gov/federal-spending-guide)
- [Basic USAspending API Training](https://www.usaspending.gov/data/Basic-API-Training.pdf)
- [GitHub: usaspending-api](https://github.com/fedspendingtransparency/usaspending-api)
- [GSDM Data Model](https://tfx.treasury.gov/data-transparency/gsdm)
- [USAspending Data Dictionary](https://www.usaspending.gov/data-dictionary)
- [FPDS Data Dictionary](https://www.fpds.gov/downloads/Version_1.5_specs/FPDS_DataDictionary_V15_OT.pdf)
- [FABS Data Dictionary](https://www.fpds.gov/downloads/FAADC/FAADC_Data_Dictionary_V1.2.pdf)

### Python Libraries
- [codeforamerica/usa_spending_python](https://github.com/codeforamerica/usa_spending_python)
- [bsweger/usaspending-scripts](https://github.com/bsweger/usaspending-scripts)
- [Data4Democracy/usaspending](https://github.com/Data4Democracy/usaspending)
- [unitedstates/federal_spending](https://github.com/unitedstates/federal_spending)
- [fedspendingtransparency/analytics-usaspending-website](https://github.com/fedspendingtransparency/analytics-usaspending-website)

### R Resources
- [RPubs: Querying USAspending API from RStudio](https://rpubs.com/cenuno/USAspendingAPI)
- [RStudio: Querying FY17 USAspending Data](https://rstudio-pubs-static.s3.amazonaws.com/285830_cd46d40938164fb18c36b2a171600dd1.html)

### Data Quality & Policy
- [GAO: Data Quality Issues Impact Usefulness](https://www.gao.gov/blog/usaspending.gov-offers-insight-government-spending-data-quality-and-user-awareness-impact-its-usefulness)
- [GAO-22-104702: Opportunities to Further Improve Information](https://www.gao.gov/products/gao-22-104702)
- [GAO-24-106214: Opportunities to Improve USAspending.gov Data](https://www.gao.gov/products/gao-24-106214)
- [GAO: How Reliable Is USAspending Data?](https://www.gao.gov/blog/usaspending.gov-contains-treasure-trove-information,-how-reliable-it)
- [CRS: Tracking Federal Awards](https://www.congress.gov/crs-product/R44027)
- [Treasury: Chapter 6000 Agency Reporting Requirements](https://tfx.treasury.gov/tfm/volume1/part2/chapter-6000-agency-reporting-requirements-usaspendinggov)

### Entity Resolution
- [GSA: Unique Entity ID Transition](https://www.gsa.gov/about-us/organization/federal-acquisition-service/fas-initiatives/integrated-award-environment/iae-systems-information-kit/unique-entity-id-is-here)
- [GSA: Unique Entity Identifier Update](https://www.gsa.gov/about-us/organization/federal-acquisition-service/fas-initiatives/integrated-award-environment/iae-systems-information-kit/unique-entity-identifier-update)
- [FEMA: What is UEI?](https://www.fema.gov/node/what-unique-entity-identifier-uei-and-how-it-related-system-award-management-sam)
- [SAM.gov Entity Registration](https://alpha.sam.gov/entity-registration)

### Commercial Tools
- [GovTribe: Favorite Federal Market Intelligence Tool](https://about.govexec.com/company/blog/govtribe-voted-favorite-federal-market-intelligence-tool-by-govbrew-readers-copy-2/)
- [FedScout About](https://www.fedscout.com/about)
- [Federal Compass](https://www.federalcompass.com)
- [GovSpend](https://govspend.com/)

### Additional Context
- [Data Lab: Federal Finance Visualizations](https://datalab.usaspending.gov/)
- [Treasury: Federal Spending Transparency](https://fedspendingtransparency.github.io/)
- [Hamilton Project: Tracking Federal Expenditures](https://www.hamiltonproject.org/data/tracking-federal-expenditures-in-real-time/)
- [FPDS: Welcome Page](https://www.fpds.gov/common/html/public_welcome_text.html)
- [What is FPDS?](https://www.federalcompass.com/blog/what-is-fpds)

</details>
<details>
<summary>S2: Comprehensive</summary>

# Prior Art: Existing Tools and Approaches

## Specialized Government Financial Document Parsers

### cafr-parsing (OpenTechStrategies)

**Repository**: [github.com/OpenTechStrategies/cafr-parsing](https://github.com/OpenTechStrategies/cafr-parsing)
**Language**: Python
**Status**: Active (open source)
**License**: Not specified in search results

**Description**: Automated data extraction from U.S. state Comprehensive Annual Financial Reports (CAFR).

**Key Features**:
- Template-based extraction system
- Parses CAFR PDF files → produces JSON output
- Automatic translation to multiple formats (XBRL, CSV, .xlsx)
- Manual template construction for each table structure
- Handles state-level financial reports

**Architecture**:
- `miner.py` - Core extraction tool requiring templates
- Templates directory - JSON files defining extraction patterns
- Data directory - Input CAFR files
- Results folder - Output location
- Example-output - Pre-generated samples in various formats

**Limitations**:
- Requires manual template creation for each jurisdiction/format
- Template maintenance needed when formats change
- No automatic format detection
- Focused on state-level CAFRs (not municipal/county)

**Use Case**: Best suited for organizations that can invest in template creation and maintenance for specific jurisdictions they monitor regularly.

**Sources**:
- [GitHub - OpenTechStrategies/cafr-parsing](https://github.com/OpenTechStrategies/cafr-parsing)
- [cafr-parser on Kandi](https://kandi.openweaver.com/python/melvinw/cafr-parser)

### financial-statement-pdf-extractor (whoiskatrin)

**Repository**: [github.com/whoiskatrin/financial-statement-pdf-extractor](https://github.com/whoiskatrin/financial-statement-pdf-extractor)
**Language**: Python
**Status**: Open source project

**Description**: Extracts structured information from annual/quarterly financial reports.

**Key Features**:
- Keyword-based table extraction (e.g., "Revenue", "Income")
- Extracts tables from PDF files
- Outputs to Excel format
- Designed for corporate financial statements

**Limitations**:
- Designed for corporate reports, not government fund accounting
- Simple keyword matching (may not handle government-specific terminology)
- No multi-page table handling mentioned
- Not specialized for government budget structures

**Relevance to Government Budgets**: Low - primarily designed for corporate financial statements, but techniques could be adapted.

**Source**: [GitHub - financial-statement-pdf-extractor](https://github.com/whoiskatrin/financial-statement-pdf-extractor)

### pdf-accounts (drkane)

**Repository**: [github.com/drkane/pdf-accounts](https://github.com/drkane/pdf-accounts)
**Language**: Python
**Tool**: `extract_financial_lines.py`

**Description**: Extracts financial data from PDFs of company accounts.

**Approach**:
- Pattern matching: text followed by numbers
- Targets balance sheets with current and previous year values
- Line-by-line extraction

**Limitations**:
- Designed for UK company accounts
- Simple pattern matching (not sophisticated parsing)
- No fund accounting support
- Limited to specific financial statement formats

**Relevance to Government Budgets**: Low - focuses on corporate accounts with simpler structures.

**Source**: [GitHub - pdf-accounts](https://github.com/drkane/pdf-accounts)

## General-Purpose PDF Table Extraction Libraries

These libraries are commonly used but lack government budget domain knowledge:

### Camelot

**Package**: `camelot-py`
**Documentation**: [camelot-py.readthedocs.io](https://camelot-py.readthedocs.io/)
**Language**: Python
**License**: Open source

**Strengths**:
- Best for lattice-style tables (visible borders)
- Excellent for tender category documents (scored 0.72 in comparative study)
- Superior to Tabula in lattice cases
- Configurable parameters for complex tables

**Weaknesses**:
- Last GitHub update: 5 years ago (maintenance concerns)
- Weaker in some categories vs. Tabula
- No OCR support
- Requires tuning for complex scenarios

**Performance**: Achieved highest score in Government Tender category (0.72) in 2026 comparative study.

**Sources**:
- [Best Python Libraries to Extract Tables From PDF in 2026](https://unstract.com/blog/extract-tables-from-pdf-python/)
- [Comparison with other PDF Table Extraction libraries - Camelot Wiki](https://github.com/camelot-dev/camelot/wiki/Comparison-with-other-PDF-Table-Extraction-libraries-and-tools)

### Tabula

**Language**: Java/Python (tabula-py wrapper)
**Status**: Active, free, continuing maintenance

**Strengths**:
- Outperforms in Manual, Scientific, and Patent categories
- Better table detection for stream cases (no borders)
- Actively maintained with code updates
- Free and widely used

**Weaknesses**:
- Parsing output quality issues even with good detection
- Less configurable than Camelot for complex tables

**Use Case**: Good starting point for simple tables, but may need supplementation for complex government documents.

**Sources**:
- [Best Python Libraries to Extract Tables From PDF in 2026](https://unstract.com/blog/extract-tables-from-pdf-python/)
- [Comparative Study of PDF Parsing Tools](https://arxiv.org/html/2410.09871v1)

### pdfplumber

**Package**: `pdfplumber`
**Language**: Python
**Blog**: [PDFPlumber: The Ultimate Python Library](https://www.blog.brightcoding.dev/2025/09/29/pdfplumber-the-ultimate-python-library-for-precision-pdf-table-and-text-extraction-with-visual-debugging/)

**Strengths**:
- Exceptional at extracting lines, intersections, cells, and tables
- Sophisticated table detection with tolerance settings
- Well-suited for financial documents (merged cells, multi-line entries)
- Detailed control over extraction process
- Visual debugging capabilities
- Handles balance sheets and income statements effectively

**Weaknesses**:
- No OCR support
- Less effective on OCR'd documents
- Requires more manual configuration for complex scenarios
- Steeper learning curve

**Best Use**: When tables are too complex for Tabula/Camelot, pdfplumber offers manual table area definition with fine-tuning.

**Sources**:
- [PDFPlumber Blog Post](https://www.blog.brightcoding.dev/2025/09/29/pdfplumber-the-ultimate-python-library-for-precision-pdf-table-and-text-extraction-with-visual-debugging/)
- [Extract Tables from PDF Python 2026](https://unstract.com/blog/extract-tables-from-pdf-python/)
- [Automated Financial Statement Data Extraction](https://oboe.com/learn/automated-financial-statement-data-extraction-with-python-1msoyzk/pdf-data-extraction-1e0pyim)

### Comparison Summary

| Tool | Best For | Maintenance | OCR | Complexity |
|------|----------|-------------|-----|------------|
| Camelot | Lattice tables, tenders | Stale (5 yrs) | No | Medium |
| Tabula | Stream tables, active use | Active | No | Low |
| pdfplumber | Complex financial tables | Active | No | High |

**Recommendation**: For government budgets, start with pdfplumber for its financial document strengths, fall back to Camelot for lattice tables. All lack government-specific domain knowledge.

## XBRL Parsing Libraries

XBRL (eXtensible Business Reporting Language) is used in some government financial reporting:

### tidyxbrl

**Package**: `tidyxbrl` (PyPI)
**Language**: Python

**Description**: Parses XBRL data files into dynamic structures that store underlying data succinctly.

**Source**: [tidyxbrl on PyPI](https://pypi.org/project/tidyxbrl/)

### EdgarTools

**Documentation**: [edgartools.readthedocs.io](https://edgartools.readthedocs.io/)
**Language**: Python

**Features**:
- XBRLInstance object with facts in pandas DataFrame
- Retrieves specific financial statements as DataFrames
- Designed for SEC filings but applicable to XBRL in general

**Sources**:
- [Getting XBRL - EdgarTools](https://edgartools.readthedocs.io/en/latest/getting-xbrl/)
- [Extract Financial Statements from SEC Filings](https://sec-api.io/resources/extract-financial-statements-from-sec-filings-and-xbrl-data-with-python)

### XBRLAssembler

**Package**: `XBRLAssembler` (PyPI)
**Language**: Python

**Description**: Parsing library for assembling XBRL documents from SEC into pandas DataFrames.

**Source**: [XBRLAssembler on PyPI](https://pypi.org/project/XBRLAssembler/)

### python-xbrl

**Package**: `python-xbrl` (PyPI)
**Dependencies**: beautifulsoup4, lxml, marshmallow

**Description**: Core XBRL parsing with XML processing and object serialization.

**Source**: [python-xbrl on PyPI](https://pypi.org/project/python-xbrl/)

### Brel

**Repository**: [github.com/BrelLibrary/brel](https://github.com/BrelLibrary/brel)
**Language**: Python

**Features**:
- Parse XBRL facts as Python objects or pandas DataFrames
- Parse XBRL networks as objects or DataFrames
- Parse XBRL roles similarly

**Source**: [GitHub - BrelLibrary/brel](https://github.com/BrelLibrary/brel)

### secfsdstools

**Package**: `secfsdstools` (PyPI)
**Language**: Python

**Description**: SEC.gov financial statements dataset analysis tool.

**Source**: [secfsdstools on PyPI](https://pypi.org/project/secfsdstools/)

### SEC Official Tools

**Repository**: [github.com/sec-gov/python-for-dera-financial-datasets](https://github.com/sec-gov/python-for-dera-financial-datasets)
**Language**: Python

**Description**: Official SEC Python code examples demonstrating how to read SEC Financial Statement and Notes Data Sets using pandas.

**Source**: [GitHub - SEC Python Examples](https://github.com/sec-gov/python-for-dera-financial-datasets)

**Relevance to Government Budgets**: Moderate - Some governments publish XBRL versions of financial reports. These libraries are essential when XBRL is available, but most local governments still publish PDF-only.

## Document Classification Approaches

For automatically identifying document types (CAFR, budget, financial report):

### Machine Learning Classification

**Libraries**:
- **scikit-learn**: Most accessible for beginners, comprehensive ML tools
- **TensorFlow**: Deep learning for complex document classification
- **PyTorch**: Alternative deep learning framework

**Common Algorithms**:
- Logistic Regression
- Random Forest
- Naive Bayes Classifier
- k-Nearest Neighbor (k-NN)
- Support Vector Machines (SVM) - highly accurate for document classification

**Text Vectorization**:
- **TF-IDF** (Term Frequency-Inverse Document Frequency) - represents word importance
- **Doc2Vec** - NLP tool for representing documents as fixed-length feature vectors

**Commercial Solutions**:
- **ABBYY FineReader Engine SDK**: ML-based document classification API
  - Automatic categorization into predefined classes
  - Handles PDFs with OCR integration

**Sources**:
- [Document Classification Using Python and ML](https://spotintelligence.com/2023/10/23/document-classification-python/)
- [Guide to Document Classification](https://nanonets.com/blog/document-classification/)
- [ABBYY FineReader Classification](https://www.abbyy.com/ocr-sdk/features/classification/)
- [Multi-Page Document Classification](https://towardsdatascience.com/multi-page-document-classification-using-machine-learning-and-nlp-ba6151405c03/)

**Relevance to Government Budgets**: High - Document classification is a crucial first step before applying specialized parsing. Training on government document types would enable automatic routing to appropriate parsers.

## Government Data APIs

While not parsing tools, these APIs provide structured access to government financial data:

### USAspending API

**URL**: [api.usaspending.gov](https://api.usaspending.gov/)

**Description**: Comprehensive U.S. government spending data including federal contracts, grants, geographic and agency breakdowns.

### U.S. Treasury Fiscal Data API

**URL**: [fiscaldata.treasury.gov/api-documentation](https://fiscaldata.treasury.gov/api-documentation/)

**Description**: Open-source tools delivering standardized federal finance information - debt, revenue, spending, interest rates, savings bonds.

### Data.gov / api.data.gov

**URL**: [api.data.gov](https://api.data.gov/)

**Description**: Free API management service for federal agencies, fulfilling Open Government Data Act obligations. Catalogs both raw data and APIs from across government.

**CKAN Integration**: data.gov powered by CKAN (robust open source data platform with API).

### Census Government Finance Data

**URLs**:
- [Interactive Census Database (Urban Institute)](https://datacatalog.urban.org/dataset/interactive-census-governments-state-local-finance-database)
- [Tax Policy Center Finance Data](https://state-local-finance-data.taxpolicycenter.org/)
- [Annual Survey of State and Local Government Finances](https://www.census.gov/programs-surveys/gov-finances.html)

**Description**: Statistics on revenue, expenditure, debt, and assets for state and local governments (1977-2019). Enables specialized searches and flexible data presentation.

**Sources**:
- [USAspending API](https://api.usaspending.gov/)
- [Treasury Fiscal Data API](https://fiscaldata.treasury.gov/api-documentation/)
- [api.data.gov](https://api.data.gov/)
- [Data.gov APIs](https://data.gov/developers/apis/)
- [Census Government Finance Data](https://datacatalog.urban.org/dataset/interactive-census-governments-state-local-finance-database)

**Relevance to Government Budgets**: High - These provide structured data when available, but don't solve the problem of extracting data from PDFs. Useful for validation and supplementing parsed data.

## Gap Analysis

### What Exists
✅ General PDF table extraction (Camelot, Tabula, pdfplumber)
✅ XBRL parsing (when standardized reports available)
✅ Template-based CAFR extraction (cafr-parsing, narrow scope)
✅ Document classification ML (general purpose)
✅ Government data APIs (when data already structured)

### What's Missing
❌ General-purpose government budget document parser (across jurisdictions)
❌ Automatic format detection and adaptation
❌ Fund accounting structure understanding
❌ Multi-page table continuation logic
❌ Validation against financial constraints (debits=credits, etc.)
❌ Confidence scoring for extracted values
❌ Human-in-the-loop workflows for ambiguous cases
❌ Pre-trained models for government document classification
❌ Cross-jurisdiction entity normalization

## Related Academic Work

While specific papers weren't found in this search, relevant research areas include:

- **Document layout analysis**: Understanding table structures in complex documents
- **Financial statement analysis**: Extracting structured data from financial reports
- **Government transparency**: Automated analysis of public financial disclosures
- **Template learning**: Automatically inferring extraction templates from examples
- **Semantic table understanding**: Distinguishing column types, hierarchies

**Research Opportunity**: A comprehensive solution combining PDF extraction, ML-based format detection, and fund accounting domain knowledge would be publishable and high-impact.

## Commercial Alternatives

Not thoroughly researched, but known commercial solutions include:
- **OpenGov**: Budget transparency and fiscal analysis platform (SaaS)
- **Tyler Technologies**: Government ERP with financial reporting
- **Socrata**: Open data platform (includes some parsing capabilities)
- **ClearGov**: Budget transparency and financial analysis

These are platforms, not libraries - they don't provide reusable parsing components for developers.

## Summary

The landscape is characterized by:
1. **Solid low-level tools** (PDF extraction) that lack domain knowledge
2. **One specialized tool** (cafr-parsing) that requires significant manual template work
3. **Strong XBRL ecosystem** for when standardized formats exist
4. **No general-purpose solution** for the diverse world of government budget PDFs

**Key Insight**: Building on pdfplumber + ML classification + fund accounting templates could create a high-value open source tool filling a clear gap in civic tech infrastructure.

</details>
<details>
<summary>S3: Need-Driven</summary>

# Solution Space: Approaches to Government Budget Document Parsing

## Architecture Patterns

### 1. Hybrid Template + ML Approach

**Concept**: Combine human-authored templates with machine learning to balance accuracy and adaptability.

**Components**:
- **Template Library**: Manually-created extraction rules for common formats
- **Format Classifier**: ML model identifying document type and likely template
- **Template Matcher**: Fuzzy matching to find best template for a document
- **Adaptive Extraction**: ML-based fallback when templates don't match perfectly
- **Human Review Queue**: Flag low-confidence extractions

**Strengths**:
- High accuracy for known formats (template-based)
- Graceful degradation for novel formats (ML fallback)
- Incremental improvement as templates added
- Interpretable (templates are human-readable rules)

**Weaknesses**:
- Requires initial template creation effort
- Template maintenance when formats change
- May not generalize to very different jurisdictions

**Complexity**: Moderate-High
**Time to Value**: Medium (start with key jurisdictions)

**Prior Art**: cafr-parsing uses pure templates; this adds ML layer for robustness.

---

### 2. Foundation Model + Fine-tuning Approach

**Concept**: Use large vision-language models (e.g., GPT-4V, Claude Vision, LLaVa) to understand document structure through multimodal learning.

**Components**:
- **Multimodal LLM**: Process PDF pages as images + text
- **Fine-tuned Model**: Trained on government financial documents
- **Structured Output**: Prompt engineering for JSON schema extraction
- **Validation Layer**: Check extracted data for financial consistency
- **Active Learning**: Human feedback improves model over time

**Strengths**:
- Handles novel formats without templates
- Can understand context (narratives, footnotes)
- Improves with scale (more training data → better results)
- Could handle multi-modal reasoning (tables + text)

**Weaknesses**:
- Requires significant compute for inference
- May hallucinate values (critical failure mode for financial data)
- Less interpretable than templates
- API costs for commercial models, hosting costs for open models
- Needs large training dataset of labeled government documents

**Complexity**: High
**Time to Value**: Long (requires model training/fine-tuning)

**Prior Art**: Financial document understanding platforms (Intuit's approach, referenced in search results) use similar techniques for corporate documents.

---

### 3. Rule-Based Pipeline with PDF Primitives

**Concept**: Build a sophisticated rule engine that works directly with PDF primitives (lines, text boxes, coordinates).

**Components**:
- **PDF Structure Parser**: Extract low-level layout information
- **Table Detector**: Identify table boundaries using line intersections
- **Column Inference**: Determine column types (text, currency, percentage)
- **Hierarchy Detector**: Recognize indentation, parent-child relationships
- **Multi-page Assembler**: Stitch continued tables across pages
- **Schema Mapper**: Map extracted structure to fund accounting schema

**Strengths**:
- No ML training required (rule-based)
- Deterministic behavior (easier to debug)
- Fine-grained control over extraction logic
- Can encode domain knowledge (fund accounting rules)

**Weaknesses**:
- Brittle to format variations
- Extensive rule development for each edge case
- Limited ability to handle truly novel formats
- Rules become complex and hard to maintain

**Complexity**: Moderate
**Time to Value**: Fast (for specific formats), slow (for broad coverage)

**Prior Art**: pdfplumber provides primitive-level access; this builds domain-specific rules on top.

---

### 4. Crowd-Sourced Template Marketplace

**Concept**: Create infrastructure for community-contributed extraction templates with version control and testing.

**Components**:
- **Template DSL**: Domain-specific language for defining extraction rules
- **Template Repository**: GitHub-like platform for sharing templates
- **Version Control**: Track template changes, fork/merge
- **Test Framework**: Validate templates against sample documents
- **Template Matcher**: Automatically select best template for a document
- **Contribution Incentives**: Gamification, attribution, institutional adoption

**Strengths**:
- Scales through community effort
- Rapid coverage expansion as civic tech orgs contribute
- Templates are transparent and auditable
- Enables specialization (state experts, city experts)

**Weaknesses**:
- Depends on community engagement
- Quality control challenges
- May have gaps in less-studied jurisdictions
- Governance needed for template conflicts

**Complexity**: Moderate (technical) + High (community management)
**Time to Value**: Slow initially, accelerates with community growth

**Prior Art**: OpenStreetMap (community mapping), Common Crawl (dataset contribution) show this model can work at scale.

---

### 5. Active Learning with Human-in-the-Loop

**Concept**: Start with basic extraction, ask humans to correct errors, learn from corrections.

**Components**:
- **Initial Extractor**: Simple baseline (e.g., pdfplumber + heuristics)
- **Confidence Scorer**: Estimate reliability of each extracted value
- **Review Interface**: Web UI for human verification/correction
- **Learning Engine**: Update extraction model based on corrections
- **Quality Metrics**: Track accuracy improvement over time
- **Expert Routing**: Send hard cases to domain experts, easy ones to crowd

**Strengths**:
- Continuous improvement from real-world use
- Human expertise applied where most valuable
- Handles edge cases naturally (human fixes them)
- Builds training dataset automatically

**Weaknesses**:
- Requires sustained human effort
- Slower than fully automated approaches
- Quality depends on reviewer expertise
- May plateau if similar errors recur

**Complexity**: Moderate
**Time to Value**: Fast (start with baseline), improves continuously

**Prior Art**: Label Studio, Prodigy (annotation tools) provide similar workflows.

---

## Technical Components (Cross-Cutting)

These components would be useful across multiple approaches:

### Document Classification

**Purpose**: Identify document type before applying specialized parsing.

**Approaches**:
1. **Text-based**: TF-IDF + SVM/Random Forest on document text
   - Fast, interpretable
   - Requires training data (100s of examples per class)

2. **Layout-based**: CNN on document thumbnails
   - Captures visual structure
   - More robust to OCR errors

3. **Metadata-based**: Use PDF metadata, file names, header text
   - Simple, fast
   - May be unreliable (metadata often missing/wrong)

**Classes**: CAFR, Operating Budget, Capital Budget, Performance Report, Single Audit, etc.

**Implementation**: scikit-learn (traditional ML) or transformers (modern NLP)

---

### Table Structure Understanding

**Purpose**: Identify table boundaries, columns, headers, hierarchies.

**Approaches**:
1. **Line-based detection**: Use visual lines in PDF (Camelot lattice mode)
2. **Whitespace analysis**: Infer columns from text alignment (Tabula stream mode)
3. **ML-based**: Train model to detect table regions and structure
4. **Hybrid**: Combine visual cues with text patterns

**Challenges**:
- Merged cells (spanning rows/columns)
- Multi-level headers (e.g., "FY 2023" spanning multiple columns)
- Implicit hierarchy (indentation indicates parent-child)
- Footnote markers within cells

---

### Multi-Page Table Assembly

**Purpose**: Stitch table fragments across pages into coherent structure.

**Approaches**:
1. **Header matching**: Detect repeated headers, use as page boundaries
2. **Continuation keywords**: Look for "continued", "carried forward"
3. **Schema consistency**: Verify column structure matches across pages
4. **Balance checking**: Ensure totals match (sum of rows = footer total)

**Edge Cases**:
- Headers that change slightly page-to-page
- Tables that restart mid-page
- Mixed tables and narrative on same page

---

### Fund Accounting Schema Mapping

**Purpose**: Map extracted values to standard fund accounting structure.

**Components**:
1. **Account code parser**: Recognize hierarchical codes (100-200-3450)
2. **Fund type classifier**: Identify General, Special Revenue, Capital, etc.
3. **Statement type detector**: Balance Sheet, Income Statement, Cash Flow
4. **Line item normalizer**: "Fund Balance" vs "Net Position" (same concept, different terms)

**Standards**:
- GASB (Governmental Accounting Standards Board) terminology
- GFOA (Government Finance Officers Association) best practices
- BARS (state-specific accounting manuals)

---

### Validation and Confidence Scoring

**Purpose**: Detect errors, estimate reliability of extracted data.

**Checks**:
1. **Arithmetic validation**: Row sums, column sums, cross-footing
2. **Domain constraints**: Fund balance can't be negative (usually), ratios must be reasonable
3. **Cross-statement consistency**: Balance Sheet assets = liabilities + equity
4. **Temporal consistency**: Prior year figures should match last year's report
5. **Peer comparison**: Values should be within reasonable range vs. similar jurisdictions

**Confidence Scoring**:
- OCR quality (for scanned PDFs)
- Template match strength
- Validation check pass rate
- Extraction ambiguity (multiple possible interpretations)

---

## Recommended Hybrid Architecture

Based on the solution space analysis, a practical approach would combine:

### Phase 1: Foundation (MVP)
1. **Base Layer**: pdfplumber for low-level extraction
2. **Template System**: JSON-based extraction templates (inspired by cafr-parsing)
3. **Template Library**: Start with 10-20 common formats (top US cities)
4. **Output Format**: Standardized JSON schema matching GASB structure

### Phase 2: Intelligence
1. **Format Classifier**: ML model to identify document type and likely template
2. **Fuzzy Template Matching**: Handle variations in known formats
3. **Confidence Scoring**: Flag uncertain extractions

### Phase 3: Scale
1. **Community Templates**: Platform for sharing templates
2. **Active Learning**: Human review interface for corrections
3. **Model Fine-tuning**: Train custom models on accumulated data

### Phase 4: Advanced
1. **Multi-modal Understanding**: Use vision-language models for novel formats
2. **Cross-Jurisdiction Entity Resolution**: Link agencies, vendors across jurisdictions
3. **Temporal Analysis**: Track changes across fiscal years

---

## Technical Stack Recommendations

### Core Extraction
- **pdfplumber**: Primary PDF parsing (active, mature)
- **PyMuPDF**: Alternative for low-level PDF access
- **Camelot**: Supplement for lattice tables

### Machine Learning
- **scikit-learn**: Document classification, traditional ML
- **transformers**: Modern NLP (LayoutLM for document understanding)
- **OpenAI/Anthropic API**: Multimodal LLM for complex cases

### Data Processing
- **pandas**: DataFrame manipulation, output generation
- **pydantic**: Schema validation for extracted data
- **jsonschema**: Template validation

### Testing & Validation
- **pytest**: Test framework
- **hypothesis**: Property-based testing (arithmetic checks)
- **Great Expectations**: Data quality validation

### Community Infrastructure
- **FastAPI**: Web API for extraction service
- **Streamlit**: Review interface for human corrections
- **PostgreSQL**: Store templates, extractions, corrections
- **GitHub**: Template repository, version control

---

## Complexity vs. Value Trade-offs

| Approach | Complexity | Time to Value | Scalability | Accuracy Ceiling |
|----------|-----------|---------------|-------------|------------------|
| Pure Templates | Low | Fast (for few formats) | Low (manual work) | High (for templated) |
| Hybrid Template+ML | Medium | Medium | Medium | High |
| Foundation Model | High | Slow | High | Medium (hallucinations) |
| Rule-Based Pipeline | Medium | Medium | Low | Medium |
| Crowd-Sourced | Medium+High | Slow → Fast | Very High | High (eventually) |
| Active Learning | Medium | Fast → Better | Medium | High (with effort) |

**Recommendation**: Start with Hybrid Template+ML (Phase 1-2), add Community Templates and Active Learning for scale (Phase 3), reserve Foundation Models for edge cases (Phase 4).

---

## Open Questions for Community Input

1. **Jurisdiction Scope**: Start US-only, or international from beginning?
2. **Template Format**: JSON, YAML, custom DSL?
3. **Hosting Model**: Self-hosted library, cloud API, or both?
4. **License**: MIT, Apache 2.0, AGPL (to prevent commercial capture)?
5. **Governance**: Who decides on schema standards, template acceptance?
6. **Funding**: Grant-funded development, volunteer-driven, or commercial support?

---

## Success Metrics

A successful solution would achieve:

**Technical**:
- `>95`% accuracy on top 100 US city CAFRs
- `<5` seconds per page extraction time
- Support for 500+ jurisdiction formats (via templates)
- Open source with active community (`>50` contributors)

**Adoption**:
- Used by 10+ civic tech organizations
- Cited in 20+ academic papers
- Integrated into 3+ commercial transparency platforms
- 100+ jurisdictions' data available in standardized format

**Impact**:
- Reduces civic tech project startup time from weeks to hours
- Enables previously impossible cross-jurisdiction analyses
- Increases government transparency through better tooling
- Creates dataset for further civic tech research

---

## Related Work to Explore

Technical papers and projects worth investigating:
1. **LayoutLM** (Microsoft Research) - Document understanding with layout
2. **TableBank** - Large-scale dataset for table detection/recognition
3. **DocVQA** - Visual question answering on documents
4. **Open Civic Data Standard** - Standardization efforts
5. **XBRL-GL** - Global Ledger taxonomy for accounting data

Academic venues publishing related work:
- ACM Conference on Knowledge Discovery and Data Mining (KDD)
- International Conference on Document Analysis and Recognition (ICDAR)
- Civic tech conferences (Code for America Summit, Personal Democracy Forum)
- Public finance journals (Municipal Finance Journal, Government Finance Review)

</details>
<details>
<summary>S4: Strategic</summary>

# Selection Criteria: Choosing Budget Document Parsing Tools

## Decision Framework

### Use Case Categories

Different users have different needs. Choose tools based on your scenario:

#### 1. Quick Prototype / One-Off Analysis

**Characteristics**:
- Need data from 1-5 jurisdictions
- One-time or infrequent extraction
- Manual verification acceptable
- Limited development time

**Recommended Approach**:
- **Tool**: pdfplumber or Tabula
- **Method**: Manual table identification, save scripts
- **Validation**: Manual review in spreadsheet
- **Time Investment**: 2-8 hours per jurisdiction

**Rationale**: General-purpose tools are good enough when scale is small. Learning curve is low, widely documented.

**Example**: Journalist analyzing one city's budget for a story.

---

#### 2. Multi-Jurisdiction Comparison

**Characteristics**:
- Need data from 10-50 jurisdictions
- Recurring updates (annual reports)
- Tolerance for some manual work
- Want reproducible process

**Recommended Approach**:
- **Tool**: cafr-parsing (OpenTechStrategies) + custom templates
- **Method**: Invest in template creation for each format family
- **Validation**: Automated arithmetic checks + spot audits
- **Time Investment**: 1-2 days per format family, reusable

**Rationale**: Template-based extraction offers good accuracy-effort trade-off at this scale. Initial investment pays off with reuse.

**Example**: Academic researcher comparing fiscal health across county governments.

---

#### 3. Large-Scale Data Aggregation

**Characteristics**:
- Need data from 100+ jurisdictions
- Continuous updates over years
- Low tolerance for errors (public-facing)
- Substantial development resources

**Recommended Approach**:
- **Tool**: Custom hybrid system (template + ML)
- **Method**: Combination of templates, format classification, active learning
- **Validation**: Multi-layer (arithmetic, temporal, peer comparison)
- **Time Investment**: 6-12 months development, ongoing maintenance

**Rationale**: At scale, investing in sophisticated infrastructure is cost-effective. Community template contributions can reduce burden.

**Example**: Civic tech nonprofit building national budget transparency platform.

---

#### 4. Exploratory Research

**Characteristics**:
- Uncertain which jurisdictions/documents needed
- Evolving research questions
- Want flexibility to pivot
- Academic or experimental

**Recommended Approach**:
- **Tool**: Jupyter notebook + pdfplumber + pandas
- **Method**: Interactive exploration, save useful code snippets
- **Validation**: Visual inspection, statistical checks
- **Time Investment**: Ongoing, accumulate reusable functions

**Rationale**: Flexibility and interpretability more valuable than scale or automation. Build tools as patterns emerge.

**Example**: PhD student exploring relationship between budget transparency and civic engagement.

---

## Tool Selection Matrix

### When to Use Each Tool

| Tool | Best For | Avoid If | Skill Level |
|------|----------|----------|-------------|
| **Tabula** | Simple tables, quick extraction, lattice/stream both | Complex multi-page tables, need high accuracy | Beginner |
| **Camelot** | Lattice tables (visible borders), government tenders | Maintenance concerns, OCR'd docs | Intermediate |
| **pdfplumber** | Complex financial tables, merged cells, precise control | Simple use cases (overkill), OCR'd docs | Intermediate |
| **cafr-parsing** | State CAFRs, willing to create templates, reusable formats | One-off extraction, unfamiliar formats | Intermediate |
| **XBRL libraries** | XBRL-formatted reports, standardized data | PDF-only sources, novel formats | Intermediate |
| **Custom Hybrid** | Large scale, diverse formats, long-term investment | Limited resources, short timeline | Advanced |
| **Foundation Models** | Novel formats, multimodal reasoning, research project | Production use (hallucinations), tight budget | Advanced |

---

## Evaluation Criteria

When evaluating tools or building custom solutions, assess these dimensions:

### 1. Accuracy

**Definition**: How often does the tool extract values correctly?

**Measurement**:
- Manual verification on sample documents (10-20)
- Compare extracted values to ground truth
- Calculate precision, recall, F1 score

**Targets**:
- **90%+**: Acceptable for exploratory research
- **95%+**: Good for decision support with human review
- **98%+**: Required for public-facing transparency platforms
- **99.5%+**: Needed for compliance/audit use cases

**Trade-offs**:
- Higher accuracy requires more engineering effort
- Template-based approaches can achieve very high accuracy for known formats
- ML approaches trade some accuracy for generalization

---

### 2. Coverage

**Definition**: How many document formats/jurisdictions can the tool handle?

**Measurement**:
- Number of successfully parsed formats
- Percentage of target jurisdictions supported
- Time required to add new format

**Targets**:
- **10-20 formats**: Sufficient for regional analysis
- **50-100 formats**: Enables state-level comparisons
- **500+ formats**: National-scale transparency
- **Automatic adaptation**: Ideal but very difficult

**Trade-offs**:
- Template-based: high accuracy, limited coverage, manual work to expand
- ML-based: lower accuracy, broader coverage, requires training data
- Hybrid: balanced, but more complex to build

---

### 3. Maintenance Burden

**Definition**: How much ongoing work is needed to keep the tool working?

**Factors**:
- Format changes (jurisdictions update report templates)
- Software dependencies (library updates, breaking changes)
- Template updates (for template-based approaches)
- Model retraining (for ML approaches)

**Assessment Questions**:
- How often do formats change? (annually, rarely, constantly)
- Is there a community maintaining templates/models?
- Are dependencies actively maintained?
- Can the tool self-diagnose failures?

**Acceptable Burden**:
- **One-off project**: No maintenance needed
- **Annual updates**: ~1-2 weeks/year per 50 jurisdictions
- **Continuous platform**: Dedicated maintainer or active community

---

### 4. Transparency & Auditability

**Definition**: Can users understand and verify how extraction works?

**Importance**: Critical for government accountability applications where errors could mislead the public.

**Spectrum**:
- **Most Transparent**: Hand-written rules, template files (cafr-parsing style)
- **Intermediate**: Interpretable ML (decision trees, linear models with feature importance)
- **Least Transparent**: Deep learning, foundation models (black box)

**Requirements by Use Case**:
- **Academic research**: Need to explain methods in papers (high transparency)
- **Civic advocacy**: Community must trust the data (moderate-high)
- **Internal analysis**: Less critical if validated (moderate)
- **Exploratory**: Transparency useful for debugging (helpful but not essential)

---

### 5. Cost

**Components**:

**Development Cost**:
- Engineering time (custom tools)
- Template creation (template-based)
- Training data annotation (ML-based)
- Integration and testing

**Operational Cost**:
- Compute (local vs. cloud)
- API calls (commercial LLMs)
- Storage (raw PDFs, extracted data)
- Human review time

**Maintenance Cost**:
- Software updates
- Format updates
- Model retraining
- Community management (crowd-sourced)

**Budget Guidance**:
| Scale | Development | Annual Operations |
|-------|-------------|-------------------|
| Small (1-10 jurisdictions) | $0-5K (internal) | `<$500` |
| Medium (10-100) | $10K-50K | $2K-10K |
| Large (100-1000) | $100K-500K | $20K-100K |
| National Scale | $500K-2M | $100K-500K |

*Assumes mix of staff time and tools; grant-funded or volunteer-driven projects can reduce costs significantly.*

---

### 6. Community & Ecosystem

**Definition**: Is there an active community around the tool?

**Indicators**:
- GitHub stars, forks, contributors
- Documentation quality
- Recent updates (< 6 months)
- Responsive maintainers
- StackOverflow / forum activity
- Published papers / blog posts

**Why It Matters**:
- Better documentation and examples
- Faster bug fixes
- More templates/models shared
- Longevity (less likely to be abandoned)

**Red Flags**:
- Last commit > 2 years ago
- Maintainer unresponsive to issues
- Dependency on unmaintained libraries
- No usage examples beyond basic docs

---

## Common Pitfalls

### 1. Underestimating Format Diversity

**Mistake**: Assuming all government budgets look similar.

**Reality**: Even within one state, city budget formats vary dramatically. Font choices, column layouts, accounting codes, terminology - all differ.

**Consequence**: A tool working on 10 jurisdictions may fail on the 11th.

**Mitigation**: Test on diverse sample before committing to an approach. Budget for format-specific customization.

---

### 2. Ignoring OCR Quality

**Mistake**: Treating all PDFs as "born digital" (text-based).

**Reality**: Many government documents are scanned images (OCR required). OCR errors propagate to extraction errors.

**Consequence**: Tools like pdfplumber fail entirely on OCR PDFs (no text layer). Even with OCR, character recognition errors (~1-5%) compound.

**Mitigation**:
- Check if PDFs are text-based or image-based
- Use OCR libraries (Tesseract, Cloud OCR) for scanned docs
- Apply OCR quality checks before extraction
- Consider OCR error correction post-processing

---

### 3. Skipping Validation

**Mistake**: Trusting extracted data without verification.

**Reality**: Extraction errors are common (OCR, table misalignment, format edge cases).

**Consequence**: Publishing incorrect financial data erodes trust, could mislead policy decisions.

**Mitigation**:
- Always implement arithmetic checks (row/column sums)
- Compare to prior years (detect anomalies)
- Spot-check against original PDFs
- Show confidence scores where uncertain
- Flag values outside reasonable ranges

---

### 4. Reinventing the Wheel

**Mistake**: Building everything from scratch without checking existing tools.

**Reality**: PDF extraction is a solved problem for many common cases. The hard part is domain-specific logic, not basic PDF parsing.

**Consequence**: Wasting time reimplementing features that exist in pdfplumber, wasting budget on features that don't differentiate.

**Mitigation**:
- Start with general-purpose libraries
- Only build custom tools for domain-specific gaps
- Contribute improvements back to upstream projects
- Focus on government-specific value (fund accounting, cross-jurisdiction)

---

### 5. Over-Engineering Early

**Mistake**: Building a sophisticated ML system before proving the use case.

**Reality**: Many projects only need data from 5-10 jurisdictions. Manual extraction might be faster and cheaper.

**Consequence**: Months spent on infrastructure that never gets used at scale.

**Mitigation**:
- Start simple (pdfplumber scripts)
- Prove the use case and value
- Scale infrastructure as demand grows
- MVP: extract data from 3 jurisdictions manually, show value, then automate

---

## Selection Decision Tree

```
START: Do you need budget document data?
│
├─ YES: How many jurisdictions?
│  │
│  ├─ 1-5: Use Tabula or pdfplumber directly
│  │      • Manual extraction, save scripts
│  │      • Validate manually in spreadsheet
│  │      • Time: 2-8 hours per jurisdiction
│  │
│  ├─ 10-50: Use cafr-parsing + custom templates
│  │       • Invest in template creation
│  │       • Automate extraction with templates
│  │       • Arithmetic validation, spot audits
│  │       • Time: 1-2 days per format, reusable
│  │
│  ├─ 100-500: Build hybrid system (template + ML)
│  │          • Custom template DSL
│  │          • Format classifier (ML)
│  │          • Community template repository
│  │          • Multi-layer validation
│  │          • Time: 6-12 months dev + ongoing maintenance
│  │
│  └─ 1000+: Large-scale infrastructure
│            • Foundation model integration
│            • Active learning pipeline
│            • Human-in-the-loop review
│            • Entity resolution, temporal tracking
│            • Time: 12-24 months dev + dedicated team
│
└─ NO: Is structured data available via API?
       │
       ├─ YES: Use API (USAspending, Census, etc.)
       │      • No PDF parsing needed
       │      • Focus on analysis, not extraction
       │
       └─ NO: Can you request structured data?
              • Ask jurisdiction for Excel/CSV
              • Cite open data policies
              • May be faster than parsing
```

---

## Recommended Starting Points by Persona

### Journalist / Civic Activist

**Goal**: Analyze specific budget for story/campaign

**Recommended Path**:
1. Check if data available in structured format (Excel, CSV)
2. If PDF only: Tabula (web interface, no coding required)
3. Validate key figures manually against PDF
4. Use Google Sheets for analysis

**Time**: 1-4 hours

---

### Academic Researcher

**Goal**: Comparative study across jurisdictions

**Recommended Path**:
1. Define sample (e.g., 20 cities in a region)
2. Survey format diversity (download all PDFs, manual inspection)
3. Choose tool based on format consistency:
   - Similar formats: pdfplumber + template scripts
   - Diverse formats: cafr-parsing or mix of tools
4. Build validation pipeline (arithmetic checks, visualizations)
5. Manual spot-check 10-20% of extractions

**Time**: 2-4 weeks setup, then reusable

---

### Civic Tech Developer

**Goal**: Build transparency platform for community

**Recommended Path**:
1. Start with local jurisdictions (5-10 in your metro area)
2. Use pdfplumber for extraction, save scripts
3. Build web visualization (lightweight: D3.js, Vega-Lite)
4. Prove value with local users (city council, journalists)
5. If successful, scale to more jurisdictions:
   - Add format classifier
   - Build template system
   - Create community contribution workflow

**Time**: 1-2 months MVP, 6-12 months for scale

---

### Government Auditor / Analyst

**Goal**: Benchmark against peer jurisdictions

**Recommended Path**:
1. Identify peer group (10-20 similar entities)
2. Check if peers use same accounting software (may have similar formats)
3. Use cafr-parsing templates if available for your state
4. Invest in template customization for your specific needs
5. Integrate with existing analysis workflows (Excel, Tableau, R)
6. Share templates with colleagues in other agencies

**Time**: 1-2 weeks per format family

---

### Research Lab / Grant-Funded Project

**Goal**: Build reusable infrastructure for civic tech community

**Recommended Path**:
1. Literature review (academic papers, civic tech reports)
2. Community needs assessment (survey potential users)
3. Pilot with 3-5 diverse jurisdictions (prove concept)
4. Build modular architecture:
   - PDF extraction layer (pdfplumber)
   - Template system (JSON-based)
   - Format classifier (scikit-learn)
   - Validation framework
   - Review interface (Streamlit)
5. Open source from day 1 (build community early)
6. Partner with civic tech orgs (Code for America, Sunlight Foundation)
7. Publish academic papers + tool releases

**Time**: 12-24 months with 2-4 person team

---

## Key Takeaways

1. **Match tool to scale**: Don't over-engineer for small projects, don't under-invest for large ones.

2. **Start simple, scale deliberately**: Manual extraction → scripts → templates → ML is a valid progression. Don't skip steps.

3. **Validate aggressively**: Financial data errors are costly. Always implement arithmetic checks and spot-check against PDFs.

4. **Consider maintenance**: Template-based approaches require ongoing updates when formats change. Plan for this.

5. **Leverage community**: Civic tech thrives on collaboration. Share your templates, contribute to open source, partner with existing projects.

6. **Know when to stop parsing**: If structured data is available (API, Excel), use it. Parsing should be last resort, not first choice.

7. **Transparency matters**: For civic applications, interpretable approaches (templates, rules) build more trust than black-box ML.

8. **Budget for iteration**: First attempt rarely works perfectly. Allow time for debugging, format edge cases, validation failures.

---

## Resources for Further Learning

### PDF Parsing Techniques
- [pdfplumber documentation](https://github.com/jsvine/pdfplumber)
- [Comparative Study of PDF Parsing Tools (2024)](https://arxiv.org/html/2410.09871v1)
- [Best PDF Table Extraction Libraries (2026)](https://unstract.com/blog/extract-tables-from-pdf-python/)

### Document Classification
- [Document Classification Guide](https://nanonets.com/blog/document-classification/)
- [scikit-learn text classification](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)

### Government Finance
- [GASB Website](https://gasb.org/) (Governmental Accounting Standards)
- [GFOA Best Practices](https://www.gfoa.org/) (Government Finance Officers Association)
- [Open Fiscal Data Package Specification](https://specs.frictionlessdata.io/fiscal-data-package/)

### Civic Tech Community
- [Code for America](https://www.codeforamerica.org/)
- [Sunlight Foundation](https://sunlightfoundation.com/) (archived resources)
- [Open Knowledge Foundation](https://okfn.org/)

### Related Research
- [OpenStreetMap](https://www.openstreetmap.org/) (example of successful community-driven data project)
- [Prose.io](http://prose.io/) (example of friendly contribution interface for technical content)

---

## Final Recommendation

For most users starting today:

**Immediate (this week)**:
- Use **pdfplumber** for 1-10 jurisdictions
- Manual scripts, save for reuse
- Validate manually

**Short-term (this month)**:
- Evaluate **cafr-parsing** templates for your region
- Build validation framework (arithmetic checks)
- Start cataloging format variations

**Long-term (this year)**:
- If scaling beyond 50 jurisdictions, invest in hybrid system
- Build or join community template repository
- Consider academic partnership for ML research

The tooling landscape is immature but improving. By starting simple and sharing what you build, you can contribute to a better civic tech ecosystem.

</details>

